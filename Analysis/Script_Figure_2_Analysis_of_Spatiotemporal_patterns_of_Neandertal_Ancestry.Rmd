---
title: "Analysis of Spatiotemporal patterns of Neandertal ancestry"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analysis of Spatiotemporal patterns of Neandertal ancestry

Here we focus on non-African individuals and how the sharing of their Neandertal segments compares the the overall population structure.

## Functions

```{r eval=T,echo=F,results='hide'}
source("Analysis_R_functions.R")
Get_Pop_masked_multiinter_fn <- function(Meta_info,All_rle,suffix,cluster_used){
  Pop_bed_names_PD = c()
  for(pop_ in unique(Meta_info[[cluster_used]])){
    
    print(pop_)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,Meta_info[,c("sample_name",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter((!!sym(cluster_used)) == pop_) %>% group_overlaps_fn(.)  %>% 
      group_by(chrom,group_id) %>% dplyr::summarise(start_pos = min(pos),end_pos = max(pos_end)) %>% 
      mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
      mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$group_id),file =paste("~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/",pop_,"_NEA_ancestry_unmasked",suffix,".bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names_PD = c(Pop_bed_names_PD,paste("~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/",pop_,"_NEA_ancestry_unmasked",suffix,".bed",sep=""))
    
  }
  
  
  system(paste0("bedtools multiinter -i ",paste(Pop_bed_names_PD, collapse=' ',sep = " ")," > ~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/Pop_NEA_ancestry_unmasked",suffix,".bed"))
  
  system(paste0("bedtools subtract -a ~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/Pop_NEA_ancestry_unmasked",suffix,".bed -b ~/EMH_Introgression_Project/Introgression_Detection/ref/bed/AA_Mask_joint.bed  > ~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/Pop_NEA_ancestry_masked",suffix,".bed
  "))
  
  Pop_masked_multiinter_all_combinations <- read.table(paste0("~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/Pop_NEA_ancestry_masked",suffix,".bed"),col.names = c("chrom","start","end","n_overlaps","combination",unique(Meta_info$clusterF3D))) %>% 
    mutate(seg_length = end - start) %>%
    mutate(combination_names = apply(.[6:(length(.)-1)], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
    mutate(across(6:(length(.)-2), ~ ifelse(. == 1, cur_column(), 0)))  %>% 
    group_by(n_overlaps,combination_names,!!!syms(colnames(.)[6:(length(.)-2)]))  %>% 
    dplyr::summarize(Unique_segs_in_Mb = sum(as.numeric(seg_length),na.rm = T)) %>% ungroup() %>% 
    mutate(Unique_segs_in_Mb = Unique_segs_in_Mb/1e6) %>% mutate(n_overlaps = ifelse(n_overlaps == 1,"Unique","Shared")) %>%
    pivot_longer(!c(combination_names,Unique_segs_in_Mb,n_overlaps),names_to = "Pop", values_to = "Value") %>% 
    filter(Value != "0") %>% group_by(Pop,n_overlaps,combination_names) %>% dplyr::summarize(Amount_ancestry_in_Mb = sum(Unique_segs_in_Mb)) %>%
    group_by(Pop) %>%
    mutate(Amount_normalized = Amount_ancestry_in_Mb / sum(Amount_ancestry_in_Mb), Total_Nea_ancestry = sum(Amount_ancestry_in_Mb)) %>% 
    mutate(Pop = ifelse(Pop == "Siberia.Americas","Siberia&Americas",Pop),Pop = gsub( "\\.", "-", Pop))
  
  Pop_masked_multiinter_all_combinations <- Meta_info  %>% filter(sample_name %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
    dplyr::select(Cov,(!!sym(cluster_used)),prop_callable) %>% group_by((!!sym(cluster_used))) %>% 
    summarize(n_ind = n(),ave_cov = mean(Cov), sd_cov = sd(Cov),min_cov = min(Cov),max_cov = max(Cov),
              ave_prop_cal=mean(prop_callable),sd_prop_cal=sd(prop_callable),min_prop_cal=min(prop_callable),max_prop_cal=max(prop_callable))  %>%
    inner_join(Pop_masked_multiinter_all_combinations,.,by=c("Pop"={{cluster_used}})) %>%
    mutate(plot_lable = paste(Pop,"n =",n_ind,sep=" "))
  
  write.table(Pop_masked_multiinter_all_combinations,
              file = paste0(folder_path_save,"Pop_masked_multiinter_all_combinations",suffix,".txt"),quote = F,row.names = F,sep="\t")
}

Summaries_single_sample_unique_frags_fn <- function(file_path_prefix,file_path_sufix,Meta_data){
  for(i in 1:n_bootstrap){
    if(i == 1){
      Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- read.table(paste0(file_path_prefix,i,file_path_sufix),col.names = c("chrom","start","end","n_overlaps","combination",unique(Meta_data[[cluster_used]]))) %>%
        mutate(seg_length = end - start) %>%
        mutate(combination_names = apply(.[6:(length(.)-1)], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
        mutate(across(6:(length(.)-2), ~ ifelse(. == 1, cur_column(), 0)))  %>% 
        group_by(n_overlaps,combination_names,!!!syms(colnames(.)[6:(length(.)-2)]))  %>% 
        dplyr::summarize(Unique_segs_in_Mb = sum(as.numeric(seg_length),na.rm = T)) %>% ungroup() %>% 
        mutate(Unique_segs_in_Mb = Unique_segs_in_Mb/1e6) %>% mutate(n_overlaps_type = ifelse(n_overlaps == 1,"Unique","Shared")) %>%
        pivot_longer(!c(combination_names,Unique_segs_in_Mb,n_overlaps,n_overlaps_type),names_to = "Pop", values_to = "Value") %>% 
        filter(Value != "0") %>% group_by(Pop,n_overlaps,n_overlaps_type,combination_names) %>% dplyr::summarize(Amount_ancestry_in_Mb = sum(Unique_segs_in_Mb)) %>%
        group_by(Pop) %>%
        mutate(Amount_normalized = Amount_ancestry_in_Mb / sum(Amount_ancestry_in_Mb), Total_Nea_ancestry = sum(Amount_ancestry_in_Mb)) %>% 
        mutate(Pop = ifelse(Pop == "Siberia.Americas","Siberia&Americas",Pop),Pop = gsub( "\\.", "-", Pop)) %>% mutate(bootstrap = i)
    }else{
      x <- read.table(paste0(file_path_prefix,i,file_path_sufix),col.names = c("chrom","start","end","n_overlaps","combination",unique(Meta_data[[cluster_used]]))) %>%
        mutate(seg_length = end - start) %>%
        mutate(combination_names = apply(.[6:(length(.)-1)], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
        mutate(across(6:(length(.)-2), ~ ifelse(. == 1, cur_column(), 0)))  %>% 
        group_by(n_overlaps,combination_names,!!!syms(colnames(.)[6:(length(.)-2)]))  %>% 
        dplyr::summarize(Unique_segs_in_Mb = sum(as.numeric(seg_length),na.rm = T)) %>% ungroup() %>% 
        mutate(Unique_segs_in_Mb = Unique_segs_in_Mb/1e6) %>% mutate(n_overlaps_type = ifelse(n_overlaps == 1,"Unique","Shared")) %>%
        pivot_longer(!c(combination_names,Unique_segs_in_Mb,n_overlaps,n_overlaps_type),names_to = "Pop", values_to = "Value") %>% 
        filter(Value != "0") %>% group_by(Pop,n_overlaps,n_overlaps_type,combination_names) %>% dplyr::summarize(Amount_ancestry_in_Mb = sum(Unique_segs_in_Mb)) %>%
        group_by(Pop) %>%
        mutate(Amount_normalized = Amount_ancestry_in_Mb / sum(Amount_ancestry_in_Mb), Total_Nea_ancestry = sum(Amount_ancestry_in_Mb)) %>% 
        mutate(Pop = ifelse(Pop == "Siberia.Americas","Siberia&Americas",Pop),Pop = gsub( "\\.", "-", Pop)) %>% mutate(bootstrap = i)
      
      Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- rbind(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap,x)
    }
    
  }
  
  Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_summary <- Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap %>%  group_by(Pop,combination_names,n_overlaps,n_overlaps_type) %>% 
    summarize(mean_amount = mean(Amount_ancestry_in_Mb),sd_amount=sd(Amount_ancestry_in_Mb),min_amount = min(Amount_ancestry_in_Mb),max_amount = max(Amount_ancestry_in_Mb),
              mean_amount_normalized = mean(Total_Nea_ancestry),sd_amount_normalized=sd(Total_Nea_ancestry),min_amount_normalized = min(Total_Nea_ancestry),max_amount_normalized = max(Total_Nea_ancestry),
              mean_total_amount = mean(Total_Nea_ancestry),sd_total_amount=sd(Total_Nea_ancestry),min_total_amount = min(Total_Nea_ancestry),max_total_amount = max(Total_Nea_ancestry))
  
  Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_summary_annotated <- Meta_data  %>% filter(sample_name %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
    dplyr::select(Cov,(!!sym(cluster_used)),prop_callable) %>% group_by((!!sym(cluster_used))) %>% 
    summarize(n_ind = n(),ave_cov = mean(Cov), sd_cov = sd(Cov),min_cov = min(Cov),max_cov = max(Cov),
              ave_prop_cal=mean(prop_callable),sd_prop_cal=sd(prop_callable),min_prop_cal=min(prop_callable),max_prop_cal=max(prop_callable)) %>%
    inner_join(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_summary,.,by=c("Pop"={{cluster_used}})) 
  
  Single_Sample_Pop_Bootstrap_unique <- Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_summary_annotated %>%
    filter(n_overlaps == 1) %>% 
    mutate(combination_names = ifelse(combination_names == "Siberia.Americas","Siberia&Americas",combination_names),
           combination_names = gsub( "\\.", "-", combination_names)) %>%
    left_join(.,cluster_color,by=c("combination_names"={{cluster_used}})) %>%
    mutate(plot_lable = paste(combination_names,"n =",n_ind,sep=" "))
  
  return(list(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap,Single_Sample_Pop_Bootstrap_unique))
}

```

## Meta Data

```{r eval=T,echo=F,results='hide'}
############## change path  ##################
folder_path = "Paper_figures"

folder_path_data = "Paper_figures_Data/"

folder_path_Sup_Tables = "Supplements_Tables/"

path_to_dryad_downloaded_files = "Dryad_folder"

path_to_my_admixfrog_folder_ancient = "Admixfrog_res_ancient"

path_to_my_admixfrog_folder_present = "Admixfrog_res_present"
##############################################

Joint_Meta <- read.csv(paste0(path_to_dryad_downloaded_files,"Meta_Data_individuals.csv")) %>% 
  filter(sample_name != "Mota",superpopulation_cluster != "Africa") %>% filter(!str_detect(ff, pattern = "^HG")) %>% mutate(sample_name=gsub( "-", ".", sample_name))

Joint_Meta_All <- read.csv(paste0(path_to_dryad_downloaded_files,"Meta_Data_individuals.csv")) %>% 
  filter(sample_name != "Mota",superpopulation_cluster != "Africa") %>%  mutate(sample_name=gsub( "-", ".", sample_name))


cluster_color_broad_all = data.frame(clusterF3D_broad_all = c("AncientEAS","ANE","ANS","EarlyOoA","EAS","North_Africa","Oceania","postLGM-WEurHG","preLGM-WEurHG","SA","Satsurblia","Siberia&Americas" ,"Sub-Sahara-Africa","SWA","WEur","Yamnaya"),
                                 cluster_color = 
                                   c("#009292FF","#FFCC00","#6DB6FFFF","#004949FF","#924900FF","#808080","#006DDBFF","#490092FF","#DB6D00FF","#920000FF","#FFB6DBFF","#24FF24FF","#000000FF","#B66DFFFF","#FF6DB6FF","#B6DBFFFF"))

cluster_color_broad = data.frame(clusterF3D_broad = c("AncientEAS","ANE","ANS","EarlyOoA","EAS","Oceania","postLGM-WEurHG","preLGM-WEurHG","SA","Satsurblia","Siberia&Americas" ,"SWA","WEur","Yamnaya"),
                                  cluster_color = 
                                    c("#009292FF","#FFCC00","#6DB6FFFF","#004949FF","#924900FF","#006DDBFF","#490092FF","#DB6D00FF","#920000FF","#FFB6DBFF","#24FF24FF","#B66DFFFF","#FF6DB6FF","#B6DBFFFF"))

cluster_color_refined = data.frame(clusterF3D_refined = c("AncientEAS","ANE","ANS","EarlyOoA","EAS","EHG","IUP","Oceania","preLGM-WEurHG","SA","Satsurblia","Siberia&Americas" ,"SWA","WEur","WHG","Yamnaya"),
                                 cluster_color = c("#009292FF","#6DB6FFFF","#004949FF","#924900FF","#DB6D00FF","#808080","#920000FF","#006DDBFF","#FFB6DBFF","#24FF24FF","#000000FF","#B66DFFFF","#FF6DB6FF","#490092FF","#B6DBFFFF","#FFCC00"))


Supercluster_color = data.frame(superpopulation_cluster = c("Americas&Asia","EarlyOoA","Oceania","WEurCAS"),Supercluster_color = c("#924900FF","#004949FF","#006DDBFF","#FF6DB6FF"))

cluster_color = cluster_color_broad
```

## Cutoffs

We use the usual cutoffs described in the paper which are a minimum length of 0.2 cM for ancient and 0.05 cM for present day individuals.

```{r eval=T,echo=F,results='hide'}
chrom_ <- c("1" ,"2" , "3" , "4",  "5" , "6"  ,"7" , "8" , "9" , "10" ,"11", "12" ,"13","14" ,"15", "16" ,"17" ,"18", "19" , "20" ,"21", "22" )

cluster_used = "population_cluster"

cluster_color = cluster_color_broad

genetic_map = "Shared_map"

min_len_present = 0.05
min_bp_len_present = 0
min_len_ancient = 0.2
min_bp_len_ancient = 0
min_SNP = 0
min_all_SNP = 0
penalty = 0.25
states = c("NEA")
called_type = as.vector(c("state"))
```


## Neandertal Ancestry PCA

First we construct a PCS from the overlaps of NEandertal segments between individuals. We use bedtools multiintersect to obtain a matrix of 0 and 1 that indicates if an individual has a Neandertal segment or not. There for we write bed file from the ALL_called_ancestry_segments.csv file from the Dryad repo.

### Get segment overlap matrix

```{r eval=T,echo=F,results='hide'}
EMH_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean > 0]
Present_Day_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean == 0,]

rle_file = read.csv(paste0(path_to_dryad_downloaded_files,"/ALL_called_ancestry_segments.csv")) %>%
  filter(genetic_map == !!genetic_map)

rle_EMH = rle_file %>% filter(sample %in% EMH_sample_list)

rle_PD = rle_file %>% filter(sample %in% Present_Day_sample_list)

EMH_file_names_NEA_state = write_bed_from_All_rle(rle_file = rle_EMH,type_ = called_type,target_ = states,chrom_ = chrom_,path = paste0("Bed_files/",genetic_map,"/"),min_len = min_len_ancient,min_bp_len = min_bp_len_ancient,min_SNP = min_SNP,optional_col = "frag_ID")

SGDP_file_names_NEA_state = write_bed_from_All_rle(rle_file = rle_PD,type_ = called_type,target_ = states,chrom_ = chrom_,path = paste0("Bed_files/",genetic_map,"/"),min_len = min_len_present,min_bp_len = min_bp_len_present,min_SNP = min_SNP,optional_col = "frag_ID")


system(paste("bedtools multiinter -i ",paste(c(EMH_file_names_NEA_state,SGDP_file_names_NEA_state), collapse=' ',sep = " ")," > Bed_files/All_NEA_state_with_indcount_unmasked_matrix.bed",sep=" "))

system(past0("bedtools subtract -a Bed_files/All_NEA_state_with_indcount_unmasked_matrix.bed  -b ",path_to_dryad_downloaded_files,"/non_callable_regions_in_physical_positions.csv > PCA_Neandertal_Ancestry/All_NEA_state_with_indcount_masked_matrix.bed
"))


sample_names <- c()
for(i in c(EMH_file_names_NEA_state)){
  smp_name = strsplit(x = tail(unlist(strsplit(i,split = "/")),1),split = "_NEA")[[1]][1]
  sample_names <- c(sample_names,smp_name)
}

for(i in c(SGDP_file_names_NEA_state)){
  smp_name = strsplit(x = tail(unlist(strsplit(i,split = "/")),1),split = "_NEA")[[1]][1]
  sample_names <- c(sample_names,smp_name)
}

write.table(x = sample_names,file = "PCA_Neandertal_Ancestry/All_NEA_state_with_indcount_unmasked_matrix_sample_names.txt",quote = F,row.names = F,col.names = F)

Nea_ancestry_ind_count_matrix_unmasked <- read.table("PCA_Neandertal_Ancestry/All_NEA_state_with_indcount_unmasked_matrix.bed",col.names = c("chrom","start","end","n_ind","ind_index",sample_names))

write.table(Nea_ancestry_ind_count_matrix_unmasked,"PCA_Neandertal_Ancestry/All_NEA_state_with_indcount_unmasked_matrix.csv",quote = F,row.names = F)

Nea_ancestry_ind_count_matrix <- read.table("PCA_Neandertal_Ancestry/All_NEA_state_with_indcount_masked_matrix.bed",col.names = c("chrom","start","end","n_ind","ind_index",sample_names))

write.table(Nea_ancestry_ind_count_matrix,"PCA_Neandertal_Ancestry/All_NEA_state_with_indcount_masked_matrix.csv",quote = F,row.names = F)

rm(Nea_ancestry_ind_count_matrix_unmasked,Nea_ancestry_ind_count_matrix)
```


### Select individuals

```{r eval=T,echo=F,results='hide'}
Segment_cluster_data <- read.table("PCA_Neandertal_Ancestry/All_NEA_state_with_indcount_masked_matrix.csv",sep=" ", header=T)

samples_x <- Joint_Meta  %>% filter(superpopulation_cluster != "Africa") %>% .$sample_name

Segment_cluster_data_red <- Segment_cluster_data %>% dplyr::select(chrom,start,end,n_ind,ind_index,!!c(samples_x))
```

### Calculate PCA

Here we calculate the PCA but only the first 5 PCs to save some time

```{r eval=T,echo=F,results='hide'}
seg.pca <- prcomp(t(Segment_cluster_data_red[,c(6:length(Segment_cluster_data_red))]), center = TRUE,rank. = 5)  

Segment_PCA_all_anno <- as.data.frame(seg.pca$x) %>% rownames_to_column(.,"sample_name") %>%
  inner_join(.,Joint_Meta,by=c("sample_name"="sample_name")) %>%
  filter(duplicated(sample_name) == FALSE)

Segment_PCA_all_importance <- summary(seg.pca)$importance

save(Segment_PCA_all_anno,Segment_PCA_all_importance,file = paste0(folder_path_data,"PCA_on_NEA_ancestry.RData"))

```

### Get highest loadings

We can also look st the loading of the PCs since we are interested which segments drive the differences between populations and what is the frequency of these segments in all populations. Do they also have them or are they unique to a population.

```{r eval=T,echo=F,results='hide'}
Segment_PCA_all_loadings = as.data.frame(seg.pca$rotation)

Segment_PCA_all_loadings = Segment_PCA_all_loadings %>% dplyr::select(PC1,PC2,PC3,PC4) %>% rownames_to_column(.,var = "group_id") %>% 
 cbind(.,Segment_cluster_data_red)


Segment_PCA_all_loadings$averageNEAfrag <- rowMeans(Segment_PCA_all_loadings[,11:(length(Segment_PCA_all_loadings))])

for( pop in unique(Joint_Meta[[cluster_used]][Joint_Meta$superpopulation_cluster != "Africa"])){
  samples = Joint_Meta$sample_name[Joint_Meta[[cluster_used]] == pop]
  samples = samples[which(samples %in% colnames(Segment_PCA_all_loadings))]
  mean_ <- Segment_PCA_all_loadings %>% dplyr::select(all_of(samples)) %>% rowMeans()
  Segment_PCA_all_loadings[,paste("averageNEAfrag_",pop,sep="")] <- mean_
}

Segment_PCA_all_loadings_stats <-  Segment_PCA_all_loadings %>% dplyr::select(!!colnames(Segment_PCA_all_loadings)[1:10],averageNEAfrag,starts_with("averageNEAfrag"))


mean_loading_PC1 <- mean(abs(Segment_PCA_all_loadings_stats$PC1))
sd_loading_PC1 <- sd(abs(Segment_PCA_all_loadings_stats$PC1))
mean_loading_PC2 <- mean(abs(Segment_PCA_all_loadings_stats$PC2))
sd_loading_PC2 <- sd(abs(Segment_PCA_all_loadings_stats$PC2))

Segment_PCA_all_loadings_red_PC1 <-  Segment_PCA_all_loadings_stats %>% filter(abs(PC1) > (mean_loading_PC1 + sd_loading_PC1 * 2)) 
Segment_PCA_all_loadings_red_PC2 <-  Segment_PCA_all_loadings_stats %>% filter(abs(PC2) > (mean_loading_PC2 + sd_loading_PC2 * 2)) 

Segment_PCA_all_loadings_red_PC1_merged <- merge_consecutive_bins_fn(Segment_PCA_all_loadings_red_PC1) %>% group_by(chrom,region_start,region_end,region_len) %>%
  dplyr::summarize(mean_abs_PC1 = mean(abs(PC1)),
       across(starts_with("averageNEAfrag"), ~ sum((end - start) * .) / mean(region_len), .names = "{.col}")
  )


Segment_PCA_all_loadings_red_PC2_merged <- merge_consecutive_bins_fn(Segment_PCA_all_loadings_red_PC2) %>% group_by(chrom,region_start,region_end,region_len) %>%
  dplyr::summarize(mean_abs_PC2 = mean(abs(PC2)),
       across(starts_with("averageNEAfrag"), ~ sum((end - start) * .) / mean(region_len), .names = "{.col}")
  )

Segment_PCA_all_loadings_red_long_PC1 <-  Segment_PCA_all_loadings_red_PC1_merged %>%
  pivot_longer(!c(chrom,region_start,region_end,region_len,mean_abs_PC1,averageNEAfrag), names_to = "pops", values_to = "NEAfrag") %>% mutate(pops = gsub("averageNEAfrag_","",pops)) 
Segment_PCA_all_loadings_red_long_PC2 <-  Segment_PCA_all_loadings_red_PC2_merged %>%
  pivot_longer(!c(chrom,region_start,region_end,region_len,mean_abs_PC2,averageNEAfrag), names_to = "pops", values_to = "NEAfrag") %>% mutate(pops = gsub("averageNEAfrag_","",pops))


save(Segment_PCA_all_loadings_red_long_PC1,file = paste0(folder_path_data,"PCA_on_NEA_ancestry_Highest_Loadings_PC1.RData"))
save(Segment_PCA_all_loadings_red_long_PC2,file = paste0(folder_path_data,"PCA_on_NEA_ancestry_Highest_Loadings_PC2.RData"))


```

### using 1KG for PCA

```{r eval=T,echo=F,results='hide'}
EMH_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean > 0]
Present_Day_SGDP_sample_list = Joint_Meta_All$sample_name[Joint_Meta$ML_BP_Mean == 0,] %>% filter(!str_detect(sample_name, pattern = "^HG"))

subset_1kG <- Joint_Meta_All %>% filter(str_detect(sample_name, pattern = "^HG")) %>% 
  group_by(clusterF3D_broad) %>% sample_n(size = 50)

Present_Day_1KG_sample_list = subset_1kG

rle_file = read.csv(paste0(path_to_dryad_downloaded_files,"/ALL_called_ancestry_segments.csv")) %>%
  filter(genetic_map == !!genetic_map)

rle_EMH = rle_file %>% filter(sample %in% EMH_sample_list)

rle_SGDP = rle_file %>% filter(sample %in% Present_Day_SGDP_sample_list)

rle_1KG = rle_file %>% filter(sample %in% Present_Day_1KG_sample_list)

EMH_file_names_NEA_state = write_bed_from_All_rle(rle_file = rle_EMH,type_ = called_type,target_ = states,chrom_ = chrom_,path = paste0("Bed_files/",genetic_map,"/"),min_len = min_len_ancient,min_bp_len = min_bp_len_ancient,min_SNP = min_SNP,optional_col = "frag_ID")

SGDP_file_names_NEA_state = write_bed_from_All_rle(rle_file = rle_SGDP,type_ = called_type,target_ = states,chrom_ = chrom_,path = paste0("Bed_files/",genetic_map,"/"),min_len = min_len_present,min_bp_len = min_bp_len_present,min_SNP = min_SNP,optional_col = "frag_ID")

G1K_file_names_NEA_state_red = write_bed_from_All_rle(rle_file = rle_1KG,type_ = called_type,target_ = states,chrom_ = chrom_,path = paste0("Bed_files/",genetic_map,"/"),min_len = min_len_present,min_bp_len = min_bp_len_present,min_SNP = min_SNP,optional_col = "frag_ID")


system(paste("bedtools multiinter -i ",paste(c(EMH_file_names_NEA_state,SGDP_file_names_NEA_state,G1K_file_names_NEA_state_red), collapse=' ',sep = " ")," > All_NEA_state_with_indcount_unmasked_matrix_with_1kG.bed",sep=" "))

system("bedtools subtract -a All_NEA_state_with_indcount_unmasked_matrix_with_1kG.bed  -b AA_Mask_joint.bed > All_NEA_state_with_indcount_masked_matrix_with_1kG.bed
")


sample_names <- c()
for(i in c(EMH_file_names_NEA_state)){
  smp_name = strsplit(x = tail(unlist(strsplit(i,split = "/")),1),split = "_NEA")[[1]][1]
  sample_names <- c(sample_names,smp_name)
}

for(i in c(SGDP_file_names_NEA_state)){
  smp_name = strsplit(x = tail(unlist(strsplit(i,split = "/")),1),split = "_NEA")[[1]][1]
  sample_names <- c(sample_names,smp_name)
}

for(i in c(G1k_file_names_NEA_state_red)){
  smp_name = strsplit(x = tail(unlist(strsplit(i,split = "/")),1),split = "_NEA")[[1]][1]
  sample_names <- c(sample_names,smp_name)
}

Nea_ancestry_ind_count_matrix <- read.table("All_NEA_state_with_indcount_masked_matrix_with_1kG.bed",col.names = c("chrom","start","end","n_ind","ind_index",sample_names))

write.table(Nea_ancestry_ind_count_matrix,"All_NEA_state_with_indcount_masked_matrix_with_1kG.csv",quote = F,row.names = F)

Segment_cluster_data <- read.table("All_NEA_state_with_indcount_masked_matrix_with_1kG.csv",sep=" ", header=T)

samples_x <- Joint_Meta_all  %>% filter(SuperclusterF3D == "Africa") %>% .$sample_name

sample_names_red <- gsub("-",".",sample_names[sample_names %notin% gsub("\\.","-",samples_x)])

Segment_cluster_data_red <- Segment_cluster_data %>% dplyr::select(chrom,start,end,n_ind,ind_index,!!c(sample_names_red))

seg.pca <- prcomp(t(Segment_cluster_data_red[,c(6:length(Segment_cluster_data_red))]), center = TRUE,rank. = 5)  

Segment_PCA_all_anno <- as.data.frame(seg.pca$x) %>% rownames_to_column(.,"sample_name") %>%
  inner_join(.,All_Joint_Meta,by=c("sample_name"="sample_name")) %>%
  filter(duplicated(sample_name) == FALSE)

Segment_PCA_all_importance <- summary(seg.pca)$importance

Segment_PCA_all_anno_PD <- Segment_PCA_all_anno %>% filter(ML_BP_Mean == 0)

Segment_PCA_plots_all_PD <-  Segment_PCA_all_anno_PD %>%
  ggplot(data=, aes(x=PC1, y=PC2,color=clusterF3D_broad,shape=clusterF3D_broad)) +
  geom_point() +
  xlab(paste("PC1",round(Segment_PCA_all_importance[2,"PC1"] * 100,1),"%",sep=" ")) +
  ylab(paste("PC2",round(Segment_PCA_all_importance[2,"PC2"]* 100,1),"%",sep=" ")) +
  scale_color_manual(name = "",values = 1:length(unique(Segment_PCA_all_anno_PD$clusterF3D_broad))) +
  scale_shape_manual(name = "",values = 1:length(unique(Segment_PCA_all_anno_PD$clusterF3D_broad))) +
  THEME +
  theme(legend.position = "bottom",
        aspect.ratio=1/1,
        legend.text = element_text(size=9, face="bold", color = "black"),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        axis.title.y = element_text(size=12, face="bold", color = "black"))


ggsave(paste0(folder_path_save,"P_Segment_PCA_plots_all_PD.png"),Segment_PCA_plots_all_PD,device = "png",width = 8,height = 8)
```

## Unique segments

Next we look at Neandertal ancestry that is either shared or unique to a population cluster.

### Segment data

```{r echo=FALSE, eval=TRUE, results='hide'}

EMH_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean > 0]
Present_Day_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean == 0]

rle_file = read.csv(paste0(path_to_dryad_downloaded_files,"/ALL_called_ancestry_segments.csv")) %>%
  filter(genetic_map == !!genetic_map)

rle_EMH = rle_file %>% filter(sample %in% EMH_sample_list) %>% 
  filter(type == called_type,target == states,chrom %in% chrom_,map_len >= min_len_ancient, pos_len >= min_bp_len_ancient)

rle_PD = rle_file %>% filter(sample %in% Present_Day_sample_list) %>% 
  filter(type == called_type,target == states,chrom %in% chrom_,map_len >= min_len_present, pos_len >= min_bp_len_present) %>%
  mutate(sample = gsub("-", "\\.", sample))



All_rle <- rbind(rle_EMH,rle_PD) 

```

### For all samples in Population Cluster

Here we get the shared and unique Neandertal ancestry for the whole population clusters

```{r eval=T,echo=F,results='hide'}

Joint_Meta_unrelated <- Joint_Meta %>% filter(sample_name %notin% c("AfanasievoSon1","AfanasievoSon2"))

Pop_bed_names = c()
for(pop_ in unique(Joint_Meta_unrelated[[cluster_used]])){
  
  print(pop_)
  NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
    inner_join(.,Joint_Meta_unrelated[,c("sample_name","superpopulation_cluster",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
    filter((!!sym(cluster_used)) == pop_) %>% group_overlaps_fn(.)  %>% 
    group_by(chrom,group_id) %>% dplyr::summarise(start_pos = min(pos),end_pos = max(pos_end)) %>% 
  mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
  mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
  
  pop_ = gsub("&","_",pop_)
  
  write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$group_id),file =paste("Unique_NEA_Segments/",pop_,"_NEA_ancestry_unmasked.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
  Pop_bed_names = c(Pop_bed_names,paste("Unique_NEA_Segments/",pop_,"_NEA_ancestry_unmasked.bed",sep=""))
  
}

system(paste("bedtools multiinter -i ",paste(Pop_bed_names, collapse=' ',sep = " ")," > Unique_NEA_Segments/Pop_NEA_ancestry_unmasked.bed",sep=" "))

system(paste0("bedtools subtract -a Unique_NEA_Segments/Pop_NEA_ancestry_unmasked.bed -b ",path_to_dryad_downloaded_files,"/AA_Mask_joint.bed  > Unique_NEA_Segments/Pop_NEA_ancestry_masked.bed
"))
```

### For Single Sample per Population Cluster

Since the population clusters have different sizes we can controle for it by randomly sample one individuals per population and compute the unique and shared Neandertal ancestry.

```{r eval=T,echo=F,results='hide'}

Pop_bed_names = c()
for(pop_ in unique(Joint_Meta_unrelated[[cluster_used]])){
  
  print(pop_)
  pop_ind <-  sample(Joint_Meta_unrelated$sample_name[Joint_Meta[[cluster_used]] == pop_],1,F)
  NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
    inner_join(.,Joint_Meta_unrelated[,c("sample_name","superpopulation_cluster",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
    filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
  mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
  mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
  
  pop_ = gsub("&","_",pop_)
  
  write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_NEA_Segments/Single_sample_from_",pop_,"_NEA_ancestry_unmasked.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
  Pop_bed_names = c(Pop_bed_names,paste("Unique_NEA_Segments/Single_sample_from_",pop_,"_NEA_ancestry_unmasked.bed",sep=""))
  
}

system(paste("bedtools multiinter -i ",paste(Pop_bed_names, collapse=' ',sep = " ")," > Unique_NEA_Segments/Single_sample_from_Pop_NEA_ancestry_unmasked.bed",sep=" "))

system(paste0("bedtools subtract -a Unique_NEA_Segments/Single_sample_from_Pop_NEA_ancestry_unmasked.bed -b ",path_to_dryad_downloaded_files,"/AA_Mask_joint.bed  > Unique_NEA_Segments/Single_sample_from_Pop_NEA_ancestry_masked.bed
"))

Single_Sample_Pop_masked_multiinter_all_combinations <- read.table("Unique_NEA_Segments/Single_sample_from_Pop_NEA_ancestry_masked.bed",col.names = c("chrom","start","end","n_overlaps","combination",unique(Joint_Meta_unrelated[[cluster_used]]))) %>%
  mutate(combination_names = apply(.[6:18], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
  group_by(combination_names) %>% 
  mutate(seg_length = end - start) %>% summarize(Unique_segs_in_Mb = sum(seg_length)/1e6)

Single_Sample_Pop_masked_multiinter <- read.table("Unique_NEA_Segments/Single_sample_from_Pop_NEA_ancestry_masked.bed",col.names = c("chrom","start","end","n_overlaps","combination",unique(Joint_Meta_unrelated[[cluster_used]]))) %>%
  pivot_longer(!c(chrom,start,end,n_overlaps,combination),names_to = "Superpop", values_to = "count") %>% 
  filter(n_overlaps == 1, count == 1) %>% group_by(Superpop) %>% 
  mutate(seg_length = end - start) %>% summarize(Unique_segs_in_Mb = sum(seg_length)/1e6)

 
```

#### Bootstrap

To get the variance we can do the randomly sampeling of an individual from each population cluster over and over.

```{r eval=T,echo=F,results='hide'}

n_bootstrap = 100

Joint_Meta_unrelated <- Joint_Meta %>% filter(sample_name %notin% c("AfanasievoSon1","AfanasievoSon2"))


for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names = c()
  for(pop_ in unique(Joint_Meta_unrelated[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(Joint_Meta_unrelated$sample_name[Joint_Meta_unrelated[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,Joint_Meta_unrelated[,c("sample_name","superpopulation_cluster",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
    mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
    mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,".bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names = c(Pop_bed_names,paste("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,".bed",sep=""))
    
  }

  system(paste("bedtools multiinter -i ",paste(Pop_bed_names, collapse=' ',sep = " ")," > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,".bed",sep=""))
  
  system(paste0("bedtools subtract -a Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,".bed -b ",path_to_dryad_downloaded_files,"/AA_Mask_joint.bed  > Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,".bed
  "))
  system(paste0("rm ",paste(Pop_bed_names, collapse=' ',sep = " ")," Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,".bed"))
}



for(i in 1:n_bootstrap){
  if(i == 1){
    Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- read.table(paste0("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,".bed"),col.names = c("chrom","start","end","n_overlaps","combination",unique(Joint_Meta_unrelated[[cluster_used]]))) %>%
  mutate(seg_length = end - start) %>%
  mutate(combination_names = apply(.[6:(length(.)-1)], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
  mutate(across(6:(length(.)-2), ~ ifelse(. == 1, cur_column(), 0)))  %>% 
  group_by(n_overlaps,combination_names,!!!syms(colnames(.)[6:(length(.)-2)]))  %>% 
  dplyr::summarize(Unique_segs_in_Mb = sum(as.numeric(seg_length),na.rm = T)) %>% ungroup() %>% 
  mutate(Unique_segs_in_Mb = Unique_segs_in_Mb/1e6) %>% mutate(n_overlaps_type = ifelse(n_overlaps == 1,"Unique","Shared")) %>%
  pivot_longer(!c(combination_names,Unique_segs_in_Mb,n_overlaps,n_overlaps_type),names_to = "Pop", values_to = "Value") %>% 
  filter(Value != "0") %>% group_by(Pop,n_overlaps,n_overlaps_type,combination_names) %>% dplyr::summarize(Amount_ancestry_in_Mb = sum(Unique_segs_in_Mb)) %>%
  group_by(Pop) %>%
  mutate(Amount_normalized = Amount_ancestry_in_Mb / sum(Amount_ancestry_in_Mb), Total_Nea_ancestry = sum(Amount_ancestry_in_Mb)) %>% 
  mutate(Pop = ifelse(Pop == "Siberia.Americas","Siberia&Americas",Pop),Pop = gsub( "\\.", "-", Pop)) %>% mutate(bootstrap = i)
  }else{
    x <- read.table(paste0("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,".bed"),col.names = c("chrom","start","end","n_overlaps","combination",unique(Joint_Meta_unrelated[[cluster_used]]))) %>%
  mutate(seg_length = end - start) %>%
  mutate(combination_names = apply(.[6:(length(.)-1)], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
  mutate(across(6:(length(.)-2), ~ ifelse(. == 1, cur_column(), 0)))  %>% 
  group_by(n_overlaps,combination_names,!!!syms(colnames(.)[6:(length(.)-2)]))  %>% 
  dplyr::summarize(Unique_segs_in_Mb = sum(as.numeric(seg_length),na.rm = T)) %>% ungroup() %>% 
  mutate(Unique_segs_in_Mb = Unique_segs_in_Mb/1e6) %>% mutate(n_overlaps_type = ifelse(n_overlaps == 1,"Unique","Shared")) %>%
  pivot_longer(!c(combination_names,Unique_segs_in_Mb,n_overlaps,n_overlaps_type),names_to = "Pop", values_to = "Value") %>% 
  filter(Value != "0") %>% group_by(Pop,n_overlaps,n_overlaps_type,combination_names) %>% dplyr::summarize(Amount_ancestry_in_Mb = sum(Unique_segs_in_Mb)) %>%
  group_by(Pop) %>%
  mutate(Amount_normalized = Amount_ancestry_in_Mb / sum(Amount_ancestry_in_Mb), Total_Nea_ancestry = sum(Amount_ancestry_in_Mb)) %>% 
  mutate(Pop = ifelse(Pop == "Siberia.Americas","Siberia&Americas",Pop),Pop = gsub( "\\.", "-", Pop)) %>% mutate(bootstrap = i)
   
  Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- rbind(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap,x)
  }

}


write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap,paste0(folder_path_data,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap.csv"),quote = F,row.names = F,sep = "\t")

```

#### Bootstrap without Oase

```{r eval=T,echo=F,results='hide'}

n_bootstrap = 100

Joint_Meta_wo_Oase <- Joint_Meta %>% filter(sample_name != "OaseNew") %>% filter(sample_name %notin% c("AfanasievoSon1","AfanasievoSon2"))

for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names = c()
  for(pop_ in unique(Joint_Meta_wo_Oase[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(Joint_Meta_wo_Oase$sample_name[Joint_Meta_wo_Oase[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,Joint_Meta_wo_Oase[,c("sample_name","superpopulation_cluster",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
    mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
    mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap_wo_Oase",i,".bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names = c(Pop_bed_names,paste("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap_wo_Oase",i,".bed",sep=""))
    
  }

  system(paste("bedtools multiinter -i ",paste(Pop_bed_names, collapse=' ',sep = " ")," > Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap_wo_Oase",i,".bed",sep=""))
  
  system(paste0("bedtools subtract -a Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap_wo_Oase",i,".bed -b ",path_to_dryad_downloaded_files,"/AA_Mask_joint.bed  > Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap_wo_Oase",i,".bed
  "))
  system(paste0("rm ",paste(Pop_bed_names, collapse=' ',sep = " ")," Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap_wo_Oase",i,".bed"))
}



for(i in 1:n_bootstrap){
  if(i == 1){
    Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_wo_Oase <- read.table(paste0("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap_wo_Oase",i,".bed"),col.names = c("chrom","start","end","n_overlaps","combination",unique(Joint_Meta_wo_Oase[[cluster_used]]))) %>%
  mutate(seg_length = end - start) %>%
  mutate(combination_names = apply(.[6:(length(.)-1)], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
  mutate(across(6:(length(.)-2), ~ ifelse(. == 1, cur_column(), 0)))  %>% 
  group_by(n_overlaps,combination_names,!!!syms(colnames(.)[6:(length(.)-2)]))  %>% 
  dplyr::summarize(Unique_segs_in_Mb = sum(as.numeric(seg_length),na.rm = T)) %>% ungroup() %>% 
  mutate(Unique_segs_in_Mb = Unique_segs_in_Mb/1e6) %>% mutate(n_overlaps_type = ifelse(n_overlaps == 1,"Unique","Shared")) %>%
  pivot_longer(!c(combination_names,Unique_segs_in_Mb,n_overlaps,n_overlaps_type),names_to = "Pop", values_to = "Value") %>% 
  filter(Value != "0") %>% group_by(Pop,n_overlaps,n_overlaps_type,combination_names) %>% dplyr::summarize(Amount_ancestry_in_Mb = sum(Unique_segs_in_Mb)) %>%
  group_by(Pop) %>%
  mutate(Amount_normalized = Amount_ancestry_in_Mb / sum(Amount_ancestry_in_Mb), Total_Nea_ancestry = sum(Amount_ancestry_in_Mb)) %>% 
  mutate(Pop = ifelse(Pop == "Siberia.Americas","Siberia&Americas",Pop),Pop = gsub( "\\.", "-", Pop)) %>% mutate(bootstrap = i)
  }else{
    x <- read.table(paste0("Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap_wo_Oase",i,".bed"),col.names = c("chrom","start","end","n_overlaps","combination",unique(Joint_Meta_wo_Oase[[cluster_used]]))) %>%
  mutate(seg_length = end - start) %>%
  mutate(combination_names = apply(.[6:(length(.)-1)], 1, function(x) paste(names(which(x >0)),collapse = ","))) %>%
  mutate(across(6:(length(.)-2), ~ ifelse(. == 1, cur_column(), 0)))  %>% 
  group_by(n_overlaps,combination_names,!!!syms(colnames(.)[6:(length(.)-2)]))  %>% 
  dplyr::summarize(Unique_segs_in_Mb = sum(as.numeric(seg_length),na.rm = T)) %>% ungroup() %>% 
  mutate(Unique_segs_in_Mb = Unique_segs_in_Mb/1e6) %>% mutate(n_overlaps_type = ifelse(n_overlaps == 1,"Unique","Shared")) %>%
  pivot_longer(!c(combination_names,Unique_segs_in_Mb,n_overlaps,n_overlaps_type),names_to = "Pop", values_to = "Value") %>% 
  filter(Value != "0") %>% group_by(Pop,n_overlaps,n_overlaps_type,combination_names) %>% dplyr::summarize(Amount_ancestry_in_Mb = sum(Unique_segs_in_Mb)) %>%
  group_by(Pop) %>%
  mutate(Amount_normalized = Amount_ancestry_in_Mb / sum(Amount_ancestry_in_Mb), Total_Nea_ancestry = sum(Amount_ancestry_in_Mb)) %>% 
  mutate(Pop = ifelse(Pop == "Siberia.Americas","Siberia&Americas",Pop),Pop = gsub( "\\.", "-", Pop)) %>% mutate(bootstrap = i)
    
  Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_wo_Oase <- rbind(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_wo_Oase,x)
  }

}

write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_wo_Oase,paste0(folder_path_data,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_wo_Oase.csv"),quote = F,row.names = F,sep = "\t")
```

### replication using 1KG and all combination of SGDP, ancient and 1KG

```{r eval=T,echo=F,results='hide'}

EMH_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean > 0]
Present_Day_SGDP_sample_list = Joint_Meta_All$sample_name[Joint_Meta$ML_BP_Mean == 0,] %>% filter(!str_detect(sample_name, pattern = "^HG"))

Present_Day_1KG_sample_list = Joint_Meta_All$sample_name[Joint_Meta$ML_BP_Mean == 0,] %>% filter(str_detect(sample_name, pattern = "^HG"))

rle_file = read.csv(paste0(path_to_dryad_downloaded_files,"/ALL_called_ancestry_segments.csv")) %>%
  filter(genetic_map == !!genetic_map)

rle_EMH = rle_file %>% filter(sample %in% EMH_sample_list) %>% 
  filter(type == called_type,target == states,chrom %in% chrom_,map_len >= min_len_ancient, pos_len >= min_bp_len_ancient) %>%
  mutate(Data_Set = "ancient")

rle_SGDP = rle_file %>% filter(sample %in% Present_Day_SGDP_sample_list) %>% 
  filter(type == called_type,target == states,chrom %in% chrom_,map_len >= min_len_present, pos_len >= min_bp_len_present) %>%
  mutate(sample = gsub("-", "\\.", sample)) %>% mutate(Data_Set = "SGDP")

rle_1KG = rle_file %>% filter(sample %in% Present_Day_1KG_sample_list) %>% 
  filter(type == called_type,target == states,chrom %in% chrom_,map_len >= min_len_present, pos_len >= min_bp_len_present) %>%
  mutate(sample = gsub("-", "\\.", sample)) %>% mutate(Data_Set = "G1k")

All_rle <- rbind(rle_EMH,rle_SGDP,rle_1KG) 

Anotation_data <- Joint_Meta_All %>% dplyr::select(sample_name,ML_BP_Mean,population_cluster) %>% distinct(.,.keep_all = T)

All_rle_annotated <- left_join(All_rle,Anotation_data,by=c("sample" = "sample_name"))

## Unique segments including 1 kG


#### All together

Get_Pop_masked_multiinter_fn(Meta_info = All_Joint_Meta_unrelated,
                             All_rle = All_rle,
                             suffix = "_all_pops_with_1kG",
                             cluster_used=cluster_used)

Pop_masked_multiinter_all_combinations_all_together <- read.csv(paste0(folder_path_save,"Pop_masked_multiinter_all_combinations_all_pops_with_1kG.txt"),sep="\t") %>%
  mutate(Data_Combination = "ancient + 1kG + SGDP")

P_Unique_vs_Shared_all_pops <- Pop_masked_multiinter_all_combinations_all_together %>% filter(n_overlaps == "Unique") %>%
  ggplot(., aes(x = fct_reorder(plot_lable, -Amount_normalized), y = Amount_normalized)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("") +
  ylab("Proportion of unique\nNeandertal ancestry") +
  THEME +
  theme(
    axis.text = element_text(size = 8,face="bold"),
    axis.text.x = element_text(angle = 90, hjust=1,face="bold",size = 10),
    legend.position = "none") 

#ggsave(paste0(folder_path_save,"P_Unique_vs_Shared_all_pops_with_1kG.png"),P_Unique_vs_Shared_all_pops,device = "png",width = 10,height = 8)

#### ancient pops with SGDP

All_Joint_Meta_unrelated_ancient_SGDP <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean > 0 | Data_Set == "SGDP")


Get_Pop_masked_multiinter_fn(Meta_info = All_Joint_Meta_unrelated_ancient_SGDP,
                             All_rle = All_rle,
                             suffix = "_ancient_and_SGDP",
                             cluster_used=cluster_used)

Pop_masked_multiinter_all_combinations_ancient_SGDP <- read.csv(paste0(folder_path_save,"Pop_masked_multiinter_all_combinations_ancient_and_SGDP.txt"),sep="\t") %>%
  mutate(Data_Combination = "ancient + SGDP")

P_Unique_vs_Shared_ancient_and_SGDP <- Pop_masked_multiinter_all_combinations_ancient_SGDP %>% filter(n_overlaps == "Unique") %>%
  ggplot(., aes(x = fct_reorder(plot_lable, -Amount_normalized), y = Amount_normalized)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("") +
  ylab("Proportion of unique\nNeandertal ancestry") +
  THEME +
  theme(
    axis.text = element_text(size = 8,face="bold"),
    axis.text.x = element_text(angle = 90, hjust=1,face="bold",size = 10),
    legend.position = "none") 

#ggsave(paste0(folder_path_save,"P_Unique_vs_Shared_all_pops_with_1kG.png"),P_Unique_vs_Shared_all_pops,device = "png",width = 10,height = 8)



#### ancient pops with 1 kG

All_Joint_Meta_unrelated_ancient_1kG <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean > 0 | Data_Set == "G1k") %>%
  filter(clusterF3D_broad %notin% c("SWA","Oceania"))

Get_Pop_masked_multiinter_fn(Meta_info = All_Joint_Meta_unrelated_ancient_1kG,
                             All_rle = All_rle,
                             suffix = "_ancient_and_1kG",
                             cluster_used=cluster_used)

Pop_masked_multiinter_all_combinations_ancient_1kG <- read.csv(paste0(folder_path_save,"Pop_masked_multiinter_all_combinations_ancient_and_1kG.txt"),sep="\t") %>%
  mutate(Data_Combination = "ancient + 1kG")

P_Unique_vs_Shared_ancient_and_1kG <- Pop_masked_multiinter_all_combinations_ancient_1kG %>% filter(n_overlaps == "Unique") %>%
  ggplot(., aes(x = fct_reorder(plot_lable, -Amount_normalized), y = Amount_normalized)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("") +
  ylab("Proportion of unique\nNeandertal ancestry") +
  THEME +
  theme(
    axis.text = element_text(size = 8,face="bold"),
    axis.text.x = element_text(angle = 90, hjust=1,face="bold",size = 10),
    legend.position = "none") 

#ggsave(paste0(folder_path_save,"P_Unique_vs_Shared_all_pops_with_1kG.png"),P_Unique_vs_Shared_all_pops,device = "png",width = 10,height = 8)

Pop_masked_multiinter_all_combinations_all_together_combinations <- rbind(Pop_masked_multiinter_all_combinations_all_together,
                                                                          Pop_masked_multiinter_all_combinations_ancient_SGDP,
                                                                          Pop_masked_multiinter_all_combinations_ancient_1kG) %>%
  mutate(Pop_lables = factor(Pop, levels=c("EarlyOoA","preLGM-WEurHG","Oceania","EAS","WEur","SA",
                                           "ANS","AncientEAS","postLGM-WEurHG","Satsurblia","SWA","ANE","Siberia&Americas","Yamnaya")),
         Data_Combination = factor(Data_Combination, levels=c("ancient + 1kG + SGDP","ancient + SGDP","ancient + 1kG")))



P_Unique_vs_Shared_ancient_and_PD <- Pop_masked_multiinter_all_combinations_all_together_combinations %>% 
  filter(n_overlaps == "Unique") %>%
  ggplot(., aes(x = Pop_lables, y = Amount_normalized,fill= Data_Combination)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("") +
  ylab("Proportion of unique\nNeandertal ancestry") +
  THEME +
  labs(fill="Data set combination") +
  theme(
    axis.text = element_text(size = 8,face="bold"),
    axis.text.x = element_text(angle = 90, hjust=1,face="bold",size = 10),
    legend.position = "bottom")

#### only PD 1kG

All_Joint_Meta_unrelated_PD1kG <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean == 0,Data_Set == "G1k")

Get_Pop_masked_multiinter_fn(Meta_info = All_Joint_Meta_unrelated_PD1kG,
                             All_rle = All_rle,
                             suffix = "_only_PD1kG",
                             cluster_used=cluster_used)

Pop_masked_multiinter_all_combinations_PD1kG <- read.table(paste0(folder_path_save,"Pop_masked_multiinter_all_combinations_only_PD1kG.txt"),sep="\t",header=T) %>%
  mutate(Data_Combination = "1kG")

P_Unique_vs_Shared_PD1kG <- Pop_masked_multiinter_all_combinations_PD1kG %>% filter(n_overlaps == "Unique") %>%
  ggplot(., aes(x = fct_reorder(plot_lable, -Amount_normalized), y = Amount_normalized)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("") +
  ylab("Proportion of unique\nNeandertal ancestry") +
  THEME +
  theme(
    axis.text = element_text(size = 8,face="bold"),
    axis.text.x = element_text(angle = 90, hjust=1,face="bold",size = 10),
    legend.position = "none") 

#ggsave(paste0(folder_path_save,"P_Unique_vs_Shared_only_pd1kG.png"),P_Unique_vs_Shared_PD1kG,device = "png",width = 10,height = 8)

#### only PD SGDP

All_Joint_Meta_unrelated_PDSGDP <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean == 0,Data_Set == "SGDP")


Get_Pop_masked_multiinter_fn(Meta_info = All_Joint_Meta_unrelated_PDSGDP,
                             All_rle = All_rle,
                             suffix = "_only_PDSGDP",
                             cluster_used=cluster_used)

Pop_masked_multiinter_all_combinations_PDSGDP <- read.table(paste0(folder_path_save,"Pop_masked_multiinter_all_combinations_only_PDSGDP.txt"),sep="\t",header=T) %>%
  mutate(Data_Combination = "SGDP")

P_Unique_vs_Shared_PDSGDP <- Pop_masked_multiinter_all_combinations_PDSGDP %>% filter(n_overlaps == "Unique") %>%
  ggplot(., aes(x = fct_reorder(plot_lable, -Amount_normalized), y = Amount_normalized)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("") +
  ylab("Proportion of unique\nNeandertal ancestry") +
  THEME +
  theme(
    axis.text = element_text(size = 8,face="bold"),
    axis.text.x = element_text(angle = 90, hjust=1,face="bold",size = 10),
    legend.position = "none") 

#ggsave(paste0(folder_path_save,"P_Unique_vs_Shared_only_pdSGDP.png"),P_Unique_vs_Shared_PDSGDP,device = "png",width = 10,height = 8)


#### only ancient 

All_Joint_Meta_unrelated_ancient_only <- All_Joint_Meta_unrelated %>% 
  filter(ML_BP_Mean > 0)

Get_Pop_masked_multiinter_fn(Meta_info = All_Joint_Meta_unrelated_ancient_only,
                             All_rle = All_rle,
                             suffix = "_only_ancient",
                             cluster_used=cluster_used)

Pop_masked_multiinter_all_combinations_ancient <- read.table(paste0(folder_path_save,"Pop_masked_multiinter_all_combinations_only_ancient.txt"),header = T,sep="\t") %>%
  mutate(Data_Combination = "ancient")

P_Unique_vs_Shared_ancient_only <- Pop_masked_multiinter_all_combinations_ancient %>% filter(n_overlaps == "Unique") %>%
  ggplot(., aes(x = fct_reorder(plot_lable, -Amount_normalized), y = Amount_normalized)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("") +
  ylab("Proportion of unique\nNeandertal ancestry") +
  THEME +
  theme(
    axis.text = element_text(size = 8,face="bold"),
    axis.text.x = element_text(angle = 90, hjust=1,face="bold",size = 10),
    legend.position = "none") 

#ggsave(paste0(folder_path_save,"P_Unique_vs_Shared_only_ancient_only_pops_with_1kG.png"),P_Unique_vs_Shared_ancient_only,device = "png",width = 10,height = 8)

#### plot together ###

P_Unique_vs_Shared_ancient <- ggpubr::ggarrange(P_Unique_vs_Shared_ancient_and_PD,ggpubr::ggarrange(P_Unique_vs_Shared_PD1kG,P_Unique_vs_Shared_PDSGDP,
                                                                                                    P_Unique_vs_Shared_ancient_only,ncol = 3,
                                                                                                    labels = c("B","C","D"),align = "hv"),nrow=2,labels = c("A",""))

ggsave(paste0(folder_path_save,"P_Unique_vs_Shared_ALL_pops_with_1kG.png"),P_Unique_vs_Shared_ancient,device = "png",width = 10,height = 9)


Combined_table_Shared_unique_all <- rbind(Pop_masked_multiinter_all_combinations_all_together,
                                          Pop_masked_multiinter_all_combinations_ancient_SGDP,
                                          Pop_masked_multiinter_all_combinations_ancient_1kG,
                                          Pop_masked_multiinter_all_combinations_PDSGDP,
                                          Pop_masked_multiinter_all_combinations_PD1kG,
                                          Pop_masked_multiinter_all_combinations_ancient) %>% 
  filter(Amount_ancestry_in_Mb >= 5)




write.table(Combined_table_Shared_unique_all,
            paste0(folder_path_save,"All_Combined_table_Shared_unique_all_revisions.csv"),quote = F,row.names = F,sep = "\t")

## Single sample all pops

n_bootstrap = 100
for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names = c()
  for(pop_ in unique(All_Joint_Meta_unrelated[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(All_Joint_Meta_unrelated$sample_name[All_Joint_Meta_unrelated[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,All_Joint_Meta_unrelated[,c("sample_name",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
      mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
      mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_with_1kG.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names = c(Pop_bed_names,paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_with_1kG.bed",sep=""))
    
  }
  
  system(paste("bedtools multiinter -i ",paste(Pop_bed_names, collapse=' ',sep = " ")," > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_with_1kG.bed",sep=""))
  
  system(paste0("bedtools subtract -a Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_with_1kG.bed -b AA_Mask_joint.bed  > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,"_with_1kG.bed
  "))
  system(paste0("rm  Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_with_1kG.bed"))
}




Single_sample_overlap_all_pops <- Summaries_single_sample_unique_frags_fn(file_path_prefix = "Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",
                                                                          file_path_sufix = "_with_1kG.bed",Meta_data = All_Joint_Meta_unrelated)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- Single_sample_overlap_all_pops[[1]]
Single_Sample_Pop_Bootstrap_unique <- Single_sample_overlap_all_pops[[2]]

write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap,
            file = paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_all_pops_with_1kG.txt"),quote = F,row.names = F,sep="\t")
write.csv(Single_Sample_Pop_Bootstrap_unique,
          file = paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_all_pops_with_1kG.csv"),quote = F,row.names = F)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_all_pops_with_1kG.txt"),sep="\t")

pairwise.t.test_res <- pairwise.t.test(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap$Amount_ancestry_in_Mb[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap$n_overlaps==1], Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap$combination_names[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap$n_overlaps==1])

pairwise.t.test_res_pvals <- Reshape_pairwise.t.test_PopClust_fn(pairwise.t.test_res)

write.csv(pairwise.t.test_res_pvals,paste0(folder_path_save,"Pairwise_t_test_for_unique_segment_all_pops_with_1kG.csv"),quote = F,row.names = T)


Single_Sample_Pop_Bootstrap_unique <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_all_pops_with_1kG.csv")) %>%
  mutate(Data_Combination = "ancient + 1kG + SGDP")

P_Single_Unique_segments_with_bootstrap <- ggplot(Single_Sample_Pop_Bootstrap_unique, aes(x = fct_reorder(plot_lable, -mean_amount), y = mean_amount)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean_amount-sd_amount, ymax=mean_amount+sd_amount), width=.2) +
  ylab("Length in Mb") +
  xlab("") +
  THEME +
  theme(axis.text = element_text(size = 8),
        axis.text.x = element_text(size=10, face="bold", color = "black",angle = 90,hjust=1),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        legend.position = "none") 
#+scale_fill_manual(name = "Genetic Clusters",labels = cluster_color[[cluster_used]],values = cluster_color$cluster_color) 

ggsave(paste0(folder_path_save,"P_Single_Unique_segments_with_bootstrap_all_pops_with_1kG.png"),P_Single_Unique_segments_with_bootstrap,device = "png",width = 8,height = 10)

## ancient and SGDP

All_Joint_Meta_unrelated_ancient_SGDP <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean > 0 | Data_Set == "SGDP") 

n_bootstrap = 100
for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names = c()
  for(pop_ in unique(All_Joint_Meta_unrelated_ancient_SGDP[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(All_Joint_Meta_unrelated_ancient_SGDP$sample_name[All_Joint_Meta_unrelated_ancient_SGDP[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,All_Joint_Meta_unrelated_ancient_SGDP[,c("sample_name",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
      mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
      mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_SGDP.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names = c(Pop_bed_names,paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_SGDP.bed",sep=""))
    
  }
  
  system(paste("bedtools multiinter -i ",paste(Pop_bed_names, collapse=' ',sep = " ")," > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_SGDP.bed",sep=""))
  
  system(paste0("bedtools subtract -a Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_SGDP.bed -b AA_Mask_joint.bed  > Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,"_ancient_and_SGDP.bed
  "))
  system(paste0("rm  ~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_SGDP.bed"))
}


Single_sample_overlap_ancient_and_SGDP <- Summaries_single_sample_unique_frags_fn(file_path_prefix = "Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",
                                                                                  file_path_sufix = "_ancient_and_SGDP.bed",Meta_data = All_Joint_Meta_unrelated_ancient_SGDP)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- Single_sample_overlap_ancient_and_SGDP[[1]]
Single_Sample_Pop_Bootstrap_unique <- Single_sample_overlap_ancient_and_SGDP[[2]]

write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap,
            file = paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_all_pops_ancient_and_SGDP.txt"),quote = F,row.names = F,sep="\t")
write.csv(Single_Sample_Pop_Bootstrap_unique,
          file = paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_all_pops_ancient_and_SGDP.csv"),quote = F,row.names = F)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_SGDP <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_all_pops_ancient_and_SGDP.txt"),sep="\t")

pairwise.t.test_res <- pairwise.t.test(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_SGDP$Amount_ancestry_in_Mb[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_SGDP$n_overlaps==1], Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_SGDP$combination_names[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_SGDP$n_overlaps==1])

pairwise.t.test_res_pvals <- Reshape_pairwise.t.test_PopClust_fn(pairwise.t.test_res)

write.csv(pairwise.t.test_res_pvals,paste0(folder_path_save,"Pairwise_t_test_for_unique_segment_ancient_SGDP.csv"),quote = F,row.names = T)


Single_Sample_Pop_Bootstrap_unique_ancient_and_SGDP <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_all_pops_ancient_and_SGDP.csv"))  %>%
  mutate(Data_Combination = "ancient + SGDP")

P_Single_Unique_segments_with_bootstrap_ancient_and_1kG <- ggplot(Single_Sample_Pop_Bootstrap_unique_ancient_and_SGDP, aes(x = fct_reorder(plot_lable, -mean_amount), y = mean_amount)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean_amount-sd_amount, ymax=mean_amount+sd_amount), width=.2) +
  ylab("Length in Mb") +
  xlab("") +
  THEME +
  theme(axis.text = element_text(size = 8),
        axis.text.x = element_text(size=10, face="bold", color = "black",angle = 90,hjust=1),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        legend.position = "none") 

## ancient and 1kG

All_Joint_Meta_unrelated_ancient_1kG <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean > 0 | Data_Set == "G1k") %>%
  filter(clusterF3D_broad %notin% c("SWA","Oceania"))

n_bootstrap = 100
for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names = c()
  for(pop_ in unique(All_Joint_Meta_unrelated_ancient_1kG[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(All_Joint_Meta_unrelated_ancient_1kG$sample_name[All_Joint_Meta_unrelated_ancient_1kG[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,All_Joint_Meta_unrelated_ancient_1kG[,c("sample_name",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
      mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
      mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_1kG.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names = c(Pop_bed_names,paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_1kG.bed",sep=""))
    
  }
  
  system(paste("bedtools multiinter -i ",paste(Pop_bed_names, collapse=' ',sep = " ")," > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_1kG.bed",sep=""))
  
  system(paste0("bedtools subtract -a Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_1kG.bed -b AA_Mask_joint.bed  > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,"_ancient_and_1kG.bed
  "))
  system(paste0("rm  Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_and_1kG.bed"))
}


Single_sample_overlap_ancient_and_1kG <- Summaries_single_sample_unique_frags_fn(file_path_prefix = "Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",
                                                                                 file_path_sufix = "_ancient_and_1kG.bed",Meta_data = All_Joint_Meta_unrelated_ancient_1kG)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap <- Single_sample_overlap_ancient_and_1kG[[1]]
Single_Sample_Pop_Bootstrap_unique <- Single_sample_overlap_ancient_and_1kG[[2]]

write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap,
            file = paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_all_pops_ancient_and_1kG.txt"),quote = F,row.names = F,sep="\t")
write.csv(Single_Sample_Pop_Bootstrap_unique,
          file = paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_all_pops_ancient_and_1kG.csv"),quote = F,row.names = F)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_1kG <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_all_pops_ancient_and_1kG.txt"),sep="\t")

pairwise.t.test_res <- pairwise.t.test(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_1kG$Amount_ancestry_in_Mb[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_1kG$n_overlaps==1], 
                                       Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_1kG$combination_names[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_and_1kG$n_overlaps==1])

pairwise.t.test_res_pvals <- Reshape_pairwise.t.test_PopClust_fn(pairwise.t.test_res,n_row = 10)

write.csv(pairwise.t.test_res_pvals,paste0(folder_path_save,"Pairwise_t_test_for_unique_segment_ancient_1KG.csv"),quote = F,row.names = T)



Single_Sample_Pop_Bootstrap_unique_ancient_and_1kG <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_all_pops_ancient_and_1kG.csv"))  %>%
  mutate(Data_Combination = "ancient + 1kG")

P_Single_Unique_segments_with_bootstrap_ancient_and_1kG <- ggplot(Single_Sample_Pop_Bootstrap_unique_ancient_and_1kG, aes(x = fct_reorder(plot_lable, -mean_amount), y = mean_amount)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean_amount-sd_amount, ymax=mean_amount+sd_amount), width=.2) +
  ylab("Length in Mb") +
  xlab("") +
  THEME +
  theme(axis.text = element_text(size = 8),
        axis.text.x = element_text(size=10, face="bold", color = "black",angle = 90,hjust=1),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        legend.position = "none") 

## plot together

Single_Sample_Pop_Bootstrap_unique_ancient_and_PD_combinations <- rbind(Single_Sample_Pop_Bootstrap_unique,
                                                                        Single_Sample_Pop_Bootstrap_unique_ancient_and_SGDP,
                                                                        Single_Sample_Pop_Bootstrap_unique_ancient_and_1kG) %>%
  mutate(Pop_lables = factor(Pop, levels=c("EarlyOoA","preLGM-WEurHG","AncientEAS","Oceania","ANS","EAS",
                                           "SA","Siberia&Americas","SWA","WEur","postLGM-WEurHG","ANE","Satsurblia","Yamnaya")),
         Data_Combination2 = factor(Data_Combination, levels=c("ancient + 1kG + SGDP","ancient + SGDP","ancient + 1kG")))



P_Single_Sample_Pop_Bootstrap_unique_ancient_and_PD <- ggplot(Single_Sample_Pop_Bootstrap_unique_ancient_and_PD_combinations, 
                                                              aes(x = Pop_lables, y = mean_amount,fill=Data_Combination2)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_errorbar(aes(ymin=mean_amount-sd_amount, ymax=mean_amount+sd_amount), width=.2,position=position_dodge(.9)) +
  ylab("Length in Mb") +
  xlab("") +
  THEME +
  labs(fill="Data set combination") +
  theme(axis.text = element_text(size = 8),
        axis.text.x = element_text(size=10, face="bold", color = "black",angle = 90,hjust=1),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        legend.position = "bottom") 

## only 1kG

All_Joint_Meta_unrelated_PD_1kG <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean == 0,Data_Set == "G1k")

n_bootstrap = 100
for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names_PD = c()
  for(pop_ in unique(All_Joint_Meta_unrelated_PD_1kG[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(All_Joint_Meta_unrelated_PD_1kG$sample_name[All_Joint_Meta_unrelated_PD_1kG[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,All_Joint_Meta_unrelated_PD_1kG[,c("sample_name",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
      mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
      mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_only_1kG.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names_PD = c(Pop_bed_names_PD,paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_only_1kG.bed",sep=""))
  }
  
  system(paste("bedtools multiinter -i ",paste(Pop_bed_names_PD, collapse=' ',sep = " ")," > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_only_1kG.bed",sep=""))
  
  system(paste0("bedtools subtract -a Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_only_1kG.bed -b AA_Mask_joint.bed  > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,"_only_1kG.bed
    "))
  system(paste0("rm  Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_only_1kG.bed"))
}

Single_sample_overlap_PD_1kG_pops <- Summaries_single_sample_unique_frags_fn(file_path_prefix = "Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",
                                                                             file_path_sufix = "_only_1kG.bed",Meta_data =  All_Joint_Meta_unrelated_PD_1kG)
Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_1kG <- Single_sample_overlap_PD_1kG_pops[[1]]
Single_Sample_Pop_Bootstrap_unique_1kG <- Single_sample_overlap_PD_1kG_pops[[2]]

write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_1kG,
            file = paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_PD_pops_1kG.csv"),quote = F,row.names = F,sep="\t")
write.csv(Single_Sample_Pop_Bootstrap_unique_1kG,
          file = paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_PD_pops_1kG.csv"),quote = F,row.names = F)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_1kG <- read.table(paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_PD_pops_1kG.csv"),sep="\t",header=T)

pairwise.t.test_res <- pairwise.t.test(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_1kG$Amount_ancestry_in_Mb[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_1kG$n_overlaps==1], Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_1kG$combination_names[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_1kG$n_overlaps==1])

pairwise.t.test_res_pvals <- Reshape_pairwise.t.test_PopClust_fn(pairwise.t.test_res,n_row = 2,row1 = "EAS","WEur")

write.csv(pairwise.t.test_res_pvals,paste0(folder_path_save,"Pairwise_t_test_for_unique_segment_only_1KG.csv"),quote = F,row.names = T)


Single_Sample_Pop_Bootstrap_unique_1kG <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_PD_pops_1kG.csv"))

P_Single_Unique_segments_with_bootstrap_PD_1kG <- ggplot(Single_Sample_Pop_Bootstrap_unique_1kG, aes(x = fct_reorder(plot_lable, -mean_amount), y = mean_amount)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean_amount-sd_amount, ymax=mean_amount+sd_amount), width=.2) +
  ylab("Length in Mb") +
  xlab("") +
  THEME +
  theme(axis.text = element_text(size = 8),
        axis.text.x = element_text(size=10, face="bold", color = "black",angle = 90,hjust=1),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        legend.position = "none") 

ggsave(paste0(folder_path_save,"P_Single_Unique_segments_with_bootstrap_pd_SGDP_pops_with_1kG.png"),P_Single_Unique_segments_with_bootstrap_PD_1kG,device = "png",width = 8,height = 9)


## only SGDP

All_Joint_Meta_unrelated_PD_SGDP <- All_Joint_Meta_unrelated  %>%
  filter(ML_BP_Mean == 0,Data_Set == "SGDP")

n_bootstrap = 100
for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names_PD = c()
  for(pop_ in unique(All_Joint_Meta_unrelated_PD_SGDP[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(All_Joint_Meta_unrelated_PD_SGDP$sample_name[All_Joint_Meta_unrelated_PD_SGDP[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,All_Joint_Meta_unrelated_PD_SGDP[,c("sample_name",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
      mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
      mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_only_SGDP.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names_PD = c(Pop_bed_names_PD,paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_only_SGDP.bed",sep=""))
  }
  
  system(paste("bedtools multiinter -i ",paste(Pop_bed_names_PD, collapse=' ',sep = " ")," > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_only_SGDP.bed",sep=""))
  
  system(paste0("bedtools subtract -a Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_only_SGDP.bed -b AA_Mask_joint.bed  > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,"_only_SGDP.bed
    "))
  system(paste0("rm  Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_only_SGDP.bed"))
}

Single_sample_overlap_PD_SGDP_pops <- Summaries_single_sample_unique_frags_fn(file_path_prefix = "~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",
                                                                              file_path_sufix = "_only_SGDP.bed",Meta_data =  All_Joint_Meta_unrelated_PD_SGDP)
Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_SGDP <- Single_sample_overlap_PD_SGDP_pops[[1]]
Single_Sample_Pop_Bootstrap_unique_SGDP <- Single_sample_overlap_PD_SGDP_pops[[2]]

write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_SGDP,
            file = paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_PD_pops_SGDP.csv"),quote = F,row.names = F,sep="\t")
write.csv(Single_Sample_Pop_Bootstrap_unique_SGDP,
          file = paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_PD_pops_SGDP.csv"),quote = F,row.names = F)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_SGDP <- read.table(paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_PD_pops_SGDP.csv"),sep="\t",header=T)

pairwise.t.test_res <- pairwise.t.test(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_SGDP$Amount_ancestry_in_Mb[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_SGDP$n_overlaps==1], Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_SGDP$combination_names[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_SGDP$n_overlaps==1])

pairwise.t.test_res_pvals <- Reshape_pairwise.t.test_PopClust_fn(pairwise.t.test_res,n_row = 5,row1 = "EAS","WEur")

write.csv(pairwise.t.test_res_pvals,paste0(folder_path_save,"Pairwise_t_test_for_unique_segment_only_SGDP.csv"),quote = F,row.names = T)

Single_Sample_Pop_Bootstrap_unique_SGDP <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_PD_pops_SGDP.csv"))

P_Single_Unique_segments_with_bootstrap_PD_SGDP <- ggplot(Single_Sample_Pop_Bootstrap_unique_SGDP, aes(x = fct_reorder(plot_lable, -mean_amount), y = mean_amount)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean_amount-sd_amount, ymax=mean_amount+sd_amount), width=.2) +
  ylab("Length in Mb") +
  xlab("") +
  THEME +
  theme(axis.text = element_text(size = 8),
        axis.text.x = element_text(size=10, face="bold", color = "black",angle = 90,hjust=1),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        legend.position = "none") 

ggsave(paste0(folder_path_save,"P_Single_Unique_segments_with_bootstrap_pd_SGDP_pops_with_1kG.png"),P_Single_Unique_segments_with_bootstrap_PD_SGDP,device = "png",width = 8,height = 9)


## ancient only 

All_Joint_Meta_unrelated_ancient_only <- All_Joint_Meta_unrelated %>% 
  filter(ML_BP_Mean > 0)

n_bootstrap = 100
for(i in 1:n_bootstrap){
  print(paste0("Bootstrap: ",i))
  Pop_bed_names_ancient = c()
  for(pop_ in unique(All_Joint_Meta_unrelated_ancient_only[[cluster_used]])){
    
    #print(pop_)
    pop_ind <-  sample(All_Joint_Meta_unrelated_ancient_only$sample_name[All_Joint_Meta_unrelated_ancient_only[[cluster_used]] == pop_],1,F)
    NEA_Overlap_data = All_rle %>% filter(sample %notin% c("AfanasievoSon1","AfanasievoSon2")) %>%
      inner_join(.,All_Joint_Meta_unrelated_ancient_only[,c("sample_name",cluster_used,"pop","ML_BP_Mean")],by=c("sample"="sample_name")) %>%
      filter(sample == pop_ind) %>% mutate(start_pos = pos,end_pos = pos_end) %>% 
      mutate(pos_len = end_pos - start_pos) %>% mutate(chrom = ifelse(chrom == "X","23",chrom)) %>% 
      mutate_at(c('chrom'), as.numeric) %>% arrange(.,chrom,start_pos,end_pos)
    
    pop_ = gsub("&","_",pop_)
    
    write.table(data.frame(chrom=NEA_Overlap_data$chrom,start=NEA_Overlap_data$start_pos,end=NEA_Overlap_data$end_pos,name=NEA_Overlap_data$id),file =paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_ancient_only.bed",sep=""),quote = F,row.names = F,col.names = F,sep = '\t')
    Pop_bed_names_ancient = c(Pop_bed_names_ancient,paste("Unique_Segments_bootstrap/Single_sample_from_",pop_,"_NEA_ancestry_unmasked_bootstrap",i,"_ancient_only.bed",sep=""))
  }
  
  system(paste("bedtools multiinter -i ",paste(Pop_bed_names_ancient, collapse=' ',sep = " ")," > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_only.bed",sep=""))
  
  system(paste0("bedtools subtract -a Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_only.bed -b AA_Mask_joint.bed  > Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",i,"_ancient_only.bed
    "))
  system(paste0("rm  Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_unmasked_bootstrap",i,"_ancient_only.bed"))
}

Single_sample_overlap_ancients_only <- Summaries_single_sample_unique_frags_fn(file_path_prefix = "~/EMH_Introgression_Project/Introgression_Detection/Analysis/Unique_NEA_Segments/Unique_Segments_bootstrap/Single_sample_from_Pop_NEA_ancestry_masked_bootstrap",
                                                                               file_path_sufix = "_ancient_only.bed",Meta_data =  All_Joint_Meta_unrelated_ancient_only)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancients_only <- Single_sample_overlap_ancients_only[[1]]
Single_Sample_Pop_Bootstrap_unique_ancients_only <- Single_sample_overlap_ancients_only[[2]]

write.table(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancients_only,
            file = paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_only_pops_with_1kG.csv"),quote = F,row.names = F,sep="\t")
write.csv(Single_Sample_Pop_Bootstrap_unique_ancients_only,
          file = paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_ancient_only_pops_with_1kG.csv"),quote = F,row.names = F)

Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancients_only <- read.table(paste0(folder_path_save,"Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancient_only_pops_with_1kG.csv"),header = T,sep="\t")

pairwise.t.test_res <- pairwise.t.test(Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancients_only$Amount_ancestry_in_Mb[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancients_only$n_overlaps==1], Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancients_only$combination_names[Single_Sample_Pop_masked_multiinter_all_combinations_bootstrap_ancients_only$n_overlaps==1])

pairwise.t.test_res_pvals <- Reshape_pairwise.t.test_PopClust_fn(pairwise.t.test_res,n_row = 11)

write.csv(pairwise.t.test_res_pvals,paste0(folder_path_save,"Pairwise_t_test_for_unique_segment_only_ancient.csv"),quote = F,row.names = T)

Single_Sample_Pop_Bootstrap_unique_ancients_only <- read.csv(paste0(folder_path_save,"Single_Sample_Pop_Bootstrap_unique_ancient_only_pops_with_1kG.csv"))

P_Single_Unique_segments_with_bootstrap_ancient_only <- ggplot(Single_Sample_Pop_Bootstrap_unique_ancients_only, aes(x = fct_reorder(plot_lable, -mean_amount), y = mean_amount)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean_amount-sd_amount, ymax=mean_amount+sd_amount), width=.2) +
  ylab("Length in Mb") +
  xlab("") +
  THEME +
  theme(axis.text = element_text(size = 8),
        axis.text.x = element_text(size=10, face="bold", color = "black",angle = 90,hjust=1),
        axis.title.x = element_text(size=12, face="bold", color = "black"),
        legend.position = "none") 
#scale_fill_manual(name = "Genetic Clusters",labels = cluster_color[[cluster_used]],values = cluster_color$cluster_color) 

ggsave(paste0(folder_path_save,"P_Single_Unique_segments_with_bootstrap_ancient_only_pops_with_1kG.png"),P_Single_Unique_segments_with_bootstrap_ancient_only,device = "png",width = 8,height = 9)

### plot All ###


P_Single_Sample_unique_segs_All <- ggpubr::ggarrange(P_Single_Sample_Pop_Bootstrap_unique_ancient_and_PD,
                                                     ggpubr::ggarrange(P_Single_Unique_segments_with_bootstrap_PD_1kG,P_Single_Unique_segments_with_bootstrap_PD_SGDP,P_Single_Unique_segments_with_bootstrap_ancient_only,ncol = 3,labels = c("B","C","D")),
                                                     nrow = 2,labels = c("A",""))

ggsave(paste0(folder_path_save,"P_Single_Sample_unique_segs_All.png"),P_Single_Sample_unique_segs_All,device = "png",width = 8,height = 9)

### data table for all

Single_Sample_Pop_Bootstrap_unique_SGDP$Data_Combination <- "SGDP"
Single_Sample_Pop_Bootstrap_unique_1kG$Data_Combination <- "1kG"
Single_Sample_Pop_Bootstrap_unique_ancients_only$Data_Combination <- "ancient"

All_single_sample_unique_table <- rbind(Single_Sample_Pop_Bootstrap_unique_ancient_and_SGDP,
                                        Single_Sample_Pop_Bootstrap_unique,
                                        Single_Sample_Pop_Bootstrap_unique_ancient_and_1kG,
                                        Single_Sample_Pop_Bootstrap_unique_SGDP,
                                        Single_Sample_Pop_Bootstrap_unique_1kG,
                                        Single_Sample_Pop_Bootstrap_unique_ancients_only)

write.table(All_single_sample_unique_table,
            paste0(folder_path_save,"All_Combined_Single_Sample_unique_all_revisions.csv"),quote = F,row.names = F,sep = "\t")
```



## Segment Correlation vs. F3

Here we compare the correlation of the location of segments between individuals to the outgroup F3 you can calculate f3 values using the Calculate_F3_on_1240k.Rmd script.

### Data

```{r eval=T,echo=F,results='hide'}
EMH_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean > 0] 
EMH_sample_list[EMH_sample_list == "Salkhit"] <- "SalkhitArchAdm"
Present_Day_sample_list = Joint_Meta$sample_name[Joint_Meta$ML_BP_Mean == 0] 
Present_Day_sample_list = gsub("-","\\.",Present_Day_sample_list)

SGDP_NEA_bins_raw = read.csv(path_to_dryad_downloaded_files,"/ALL_NEA_bins_PP_Shared_map.csv") %>%
  dplyr:::select(chrom,map,pos,id,!!Present_Day_sample_list)

sample_list_present = Joint_Meta %>% filter(time == "present" ) %>% 
  filter(superpopulation_cluster != "Africa") %>% dplyr::select(pop) %>% distinct()


joint_by_pop = merge_pop_vectors_fn(pop.list = sample_list_present$pop,
                                    data.matrix = SGDP_NEA_bins_raw,
                                    Meta.info = Joint_Meta,
                                    BooleanCoding = F)

SGDP_NEA_bins_raw_joint_by_pop <- cbind(SGDP_NEA_bins_raw[,c(1:4)],joint_by_pop)

sample_list_ancient = Joint_Meta %>% filter(ML_BP_Mean > 0) %>% 
  filter(superpopulation_cluster != "Africa") %>% .$sample_name

EMH_NEA_bins_raw = read.csv(path_to_dryad_downloaded_files,"/ALL_NEA_bins_PP_Shared_map.csv") %>%
   dplyr::select(chrom,map,!!EMH_sample_list)

ALL_NEA_bins_raw = inner_join(SGDP_NEA_bins_raw_joint_by_pop,EMH_NEA_bins_raw,by=c("chrom","map")) 


ALL_NEA_bins_raw <- ALL_NEA_bins_raw[,c(1:4)] %>% group_by(chrom) %>% mutate(pos_end = lead(pos)) %>% ungroup() %>% 
  mutate(pos_end = ifelse(is.na(pos_end), pos + 5000,pos_end)) %>% mutate(pos_len = pos_end - pos) %>% cbind(.,ALL_NEA_bins_raw[,c(5:length(ALL_NEA_bins_raw))])

non_callable_bins <- read.csv(paste0(path_to_dryad_downloaded_files,"/non_callable_bins_Shared_map.csv"),col.names = c("chrom","map","pos","id"))

ALL_NEA_bins_raw_masked <- anti_join(ALL_NEA_bins_raw,non_callable_bins,by="id")

ALL_NEA_bins_raw_matrix <- ALL_NEA_bins_raw_masked[,c(7:length(ALL_NEA_bins_raw_masked))]

ALL_NEA_bins_raw_matrix[ALL_NEA_bins_raw_matrix > 0] <- 1

ALL_NEA_bins_raw_masked <- cbind(ALL_NEA_bins_raw_masked[,c(1:6)],ALL_NEA_bins_raw_matrix) %>% dplyr::select(!c(pos_end,pos_len))

write.csv(ALL_NEA_bins_raw_masked,paste0(folder_path_Sup_Tables,"ALL_NEA_bins_raw_masked.csv"),quote = F,row.names = T)

```

### Reducing sample size to representatives of population

To reduce the size of the samples displayed we choose representatives of each genetic population for present day individuals. For each genetic cluster we choose 2 present day individuals randomly.  All Ancient samples are used except direct genetic relatives of samples (AfanasievoSon1,AfanasievoSon2) or from the exact same site. Further more we exclude samples from Africa since their Neandertal ancestry is very low and the Mbuti population was used for the F3 calculations as an outgroup.


```{r echo=TRUE, eval=TRUE}

set.seed(42)
Present_day_sample_list = c()
present_day_inds = Joint_Meta %>% filter(ML_BP_Mean == 0,superpopulation_cluster != "Africa") %>% distinct(ID, .keep_all = TRUE)
for(clstr in unique(present_day_inds[[cluster_used]])){
  x = sample(present_day_inds$ID[present_day_inds[[cluster_used]] == clstr],ifelse(length(present_day_inds$ID[present_day_inds[[cluster_used]] == clstr])>1,2,1),replace = F)
  Present_day_sample_list <- c(Present_day_sample_list,x)
}


ancient_inds_same_site = Joint_Meta %>% filter(ML_BP_Mean > 0,superpopulation_cluster != "Africa") %>%
  mutate(Longitude = round(Longitude,2),Latitude = round(Latitude,2)) %>% 
  group_by(Longitude,Latitude) %>%
  filter(n()>1)

ancient_inds_list = Joint_Meta %>% filter(ML_BP_Mean > 0,superpopulation_cluster != "Africa") %>%
  mutate(Longitude = round(Longitude,2),Latitude = round(Latitude,2)) %>% 
  group_by(Longitude,Latitude) %>%
  filter(n()==1) %>% ungroup() %>% dplyr:: select(sample_name) %>% .$sample_name

set.seed(42)
same_site = unique(ancient_inds_same_site[,c("Longitude","Latitude")])
for(clstr in  1:length(same_site$Longitude)){
  x = sample(ancient_inds_same_site$ID[ancient_inds_same_site$Longitude == same_site$Longitude[clstr] & ancient_inds_same_site$Latitude == same_site$Latitude[clstr]],1,replace = F)
  ancient_inds_list <- c(ancient_inds_list,x)
}

All_Sample_list <- c(ancient_inds_list,Present_day_sample_list)
```

### Calculate cor

```{r echo=TRUE, eval=TRUE}
ALL_NEA_bins_raw_Autosomes <- ALL_NEA_bins_raw_masked %>% filter(chrom != "X") 

Autosomes_NEA_bins.matrix_all = ALL_NEA_bins_raw_Autosomes %>% 
  mutate(rowsum=rowSums(.[5:length(.)])) %>% distinct()

Autosomes_NEA_bins.matrix = Autosomes_NEA_bins.matrix_all[4:length(Autosomes_NEA_bins.matrix_all)] %>% 
  dplyr::select(-rowsum) %>% column_to_rownames(., var = "id")

Autosomes_NEA_bins.matrix_no_zero_cols = Autosomes_NEA_bins.matrix_all[4:length(Autosomes_NEA_bins.matrix_all)] %>% 
  select_if(function(.) sum(.) != 0) %>% dplyr::select(-rowsum) %>% column_to_rownames(., var = "id")

Autosomes_NEA_bins.cor = cor(Autosomes_NEA_bins.matrix_no_zero_cols)
diag(Autosomes_NEA_bins.cor) <- NA

write.csv(Autosomes_NEA_bins.cor,paste0(folder_path_Sup_Tables,"Autosomes_NEA_bins.cor.csv"),quote = F,row.names = T)

rm(SGDP_NEA_bins_raw,EMH_NEA_bins_raw,SGDP_NEA_bins_raw_joint_by_pop,joint_by_pop,ALL_NEA_bins_raw_Autosomes,Autosomes_NEA_bins.matrix_no_zero_cols,Autosomes_NEA_bins.matrix,Autosomes_NEA_bins.matrix_all)
```



### read in F3 values 

The F3 values are precomputed for every pairwise population using Mbuti as an outgroup. The populations for present day individuals are the SGDP assigned populations and for ancient individuals it is done per individual. For all F3 only SNPs are used where at least 50 % of the ancients have data. The F3 are either computed on the archaicadmixtureAPX set or 1240k and for autosomes and X chrom separately. You can comute thes using the Calculate_F3_on_1240k.Rmd script (change the admixfrog reference from 1240k to archaicadmixturAPX to get F3s on the archaicadmixture sites).

```{r echo=TRUE, eval=TRUE}
# archaicadmixtureAPX
## change the path!!!
res_f3_gtLH_archaicadmixtureAPX_Autosomes <- read.csv(paste0("res_f3_AF_gtLH_A1240k_",genetic_map,".csv"))
res_f3_gtLH_archaicadmixtureAPX_Autosomes[res_f3_gtLH_archaicadmixtureAPX_Autosomes == "Sahrawi"] <- "Saharawi"
res_f3_gtLH_archaicadmixtureAPX_Autosomes[res_f3_gtLH_archaicadmixtureAPX_Autosomes == "Salkhit"] <- "SalkhitArchAdm"


res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix = res_f3_gtLH_archaicadmixtureAPX_Autosomes %>% dplyr::select(c(pop2,pop3,est)) %>% pivot_wider(names_from = pop3, values_from = est) %>% column_to_rownames(.,"pop2") %>% as.matrix()
diag(res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix) <- 0
res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix_dis <- max(res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix)- res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix
diag(res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix_dis) <- 0

### change the path!!!
write.csv(res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix,paste0(folder_path_Sup_Tables,"res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix.csv"),quote = F,row.names = T)

# 1240k

res_f3_gtLH_1240k_Autosomes <- read.csv(paste0("res_f3_AF_gtLH_archaicadmixtureX_",genetic_map,".csv"))
res_f3_gtLH_1240k_Autosomes[res_f3_gtLH_1240k_Autosomes == "Sahrawi"] <- "Saharawi"
res_f3_gtLH_1240k_Autosomes[res_f3_gtLH_1240k_Autosomes == "Salkhit"] <- "SalkhitArchAdm"

res_f3_gtLH_1240k_Autosomes_matrix = res_f3_gtLH_1240k_Autosomes %>% dplyr::select(c(pop2,pop3,est)) %>% pivot_wider(names_from = pop3, values_from = est) %>% column_to_rownames(.,"pop2") %>% as.matrix()
diag(res_f3_gtLH_1240k_Autosomes_matrix) <- 0
res_f3_gtLH_1240k_Autosomes_matrix_dis <- max(res_f3_gtLH_1240k_Autosomes_matrix)- res_f3_gtLH_1240k_Autosomes_matrix
diag(res_f3_gtLH_1240k_Autosomes_matrix_dis) <- 0

write.csv(res_f3_gtLH_1240k_Autosomes_matrix,paste0(folder_path_Sup_Tables,"res_f3_gtLH_1240k_Autosomes_matrix.csv"),quote = F,row.names = T)

```

### Plot half Neacor and half 1240k matrix 

```{r echo=FALSE, eval=T}
cormat <- reorder_cormat(Autosomes_NEA_bins.cor)
no_close_relative <- colnames(cormat)[colnames(cormat) %notin% c("AfanasievoSon1","AfanasievoSon1","AfanasievoSon2","AfanasievoSon2") ]
mean_cormat <- mean(cormat[no_close_relative, no_close_relative],na.rm =T)
sd_cormat <- sd(cormat[no_close_relative, no_close_relative],na.rm =T)

cormat_standardize <- sapply(1:length(colnames(cormat)), function(i) {
  cormat[i,] <- ( cormat[i,] - mean_cormat )/ sd_cormat
})

F3_1240k_matrix <- res_f3_gtLH_1240k_Autosomes_matrix[rownames(cormat), colnames(cormat)]
mean_F3 <- mean(F3_1240k_matrix[no_close_relative, no_close_relative],na.rm =T)
sd_F3 <- sd(F3_1240k_matrix[no_close_relative, no_close_relative],na.rm =T)

F3mat_standardize <- sapply(1:length(colnames(F3_1240k_matrix)), function(i) {
  F3_1240k_matrix[i,] <- ( F3_1240k_matrix[i,] - mean_F3 )/ sd_F3
})
diag(F3mat_standardize) <- NA

matrix_combined <- cormat_standardize
matrix_combined[upper.tri(matrix_combined)] <- F3mat_standardize[upper.tri(F3mat_standardize)]
colnames(matrix_combined) <- colnames(F3_1240k_matrix)
diag(matrix_combined) <- 0
melted_combined <- reshape2::melt(matrix_combined) %>% distinct() 
melted_combined_anno = annotate_matrix(long_data_matrix = melted_combined,value_name = "joint_standardized",cluster_used = cluster_used)

melted_combined_anno_red = melted_combined_anno %>% filter(Var1 %in% All_Sample_list, Var2 %in% All_Sample_list)

df_poporder <- data.frame(roworder = rownames(cormat)[rownames(cormat) %in% All_Sample_list], colorder = colnames(cormat)[colnames(cormat) %in% All_Sample_list])

P_combined <- ggHeatmap(annotated_matrix = melted_combined_anno_red,min_limit = min(melted_combined_anno_red$joint_standardized,na.rm = T),max_limit = max(melted_combined_anno_red$joint_standardized,na.rm = T),col_value_name = "joint_standardized")

```

### Check corelation of estimates

```{r echo=FALSE, eval=TRUE}
cormat <- reorder_cormat(Autosomes_NEA_bins.cor)
melted_cormat <- reshape2::melt(cormat) %>% distinct() 

melted_cormat_anno = annotate_matrix(long_data_matrix = melted_cormat,value_name = "NEAcor",cluster_used = cluster_used)

melted_F3_AA_APX_matrix <- reshape2::melt(res_f3_gtLH_archaicadmixtureAPX_Autosomes_matrix) 
melted_F3_AA_APX_matrix <- inner_join(melted_F3_AA_APX_matrix,melted_cormat[,c("Var1","Var2")],by=c("Var1","Var2"))

melted_F3_AA_APX_anno = annotate_matrix(long_data_matrix = melted_F3_AA_APX_matrix,value_name = "F3",match_to_matrix = melted_cormat,cluster_used = cluster_used)
melted_F3_AA_APX_anno$F3 <- ifelse(melted_F3_AA_APX_anno$Var1 == melted_F3_AA_APX_anno$Var2 , NA, melted_F3_AA_APX_anno$F3)

melted_F3_1240k_matrix <- reshape2::melt(res_f3_gtLH_1240k_Autosomes_matrix)
melted_F3_1240k_matrix <- inner_join(melted_F3_1240k_matrix,melted_cormat[,c("Var1","Var2")],by=c("Var1","Var2"))

melted_F3_1240k_anno = annotate_matrix(long_data_matrix = melted_F3_1240k_matrix,value_name = "F3",match_to_matrix = melted_cormat,cluster_used = cluster_used)
melted_F3_1240k_anno$F3 <- ifelse(melted_F3_1240k_anno$Var1 == melted_F3_1240k_anno$Var2 , NA, melted_F3_1240k_anno$F3)

Joint_values <- inner_join(melted_cormat_anno,melted_F3_AA_APX_anno,by=c("Var1","Var2","Var1_cluster","Var2_cluster","Var1_ML_BP_Mean","Var2_ML_BP_Mean")) %>% distinct() %>% drop_na() %>% dplyr::select(Var1,Var2,Var1_cluster,Var2_cluster,Var1_ML_BP_Mean,Var2_ML_BP_Mean,starts_with("NEAcor"),starts_with("F3")) %>% dplyr::rename(F3_AA = F3) %>% inner_join(.,melted_F3_1240k_anno,by=c("Var1","Var2","Var1_cluster","Var2_cluster","Var1_ML_BP_Mean","Var2_ML_BP_Mean")) %>% distinct() %>% drop_na() %>% dplyr::select(Var1,Var2,Var1_cluster,Var2_cluster,Var1_ML_BP_Mean,Var2_ML_BP_Mean,starts_with("NEAcor"),starts_with("F3")) %>% dplyr::rename(F3_1240k = F3)

Joint_values$Cluster_comp <- ifelse(Joint_values$Var1_cluster == Joint_values$Var2_cluster,"inside","between")
Joint_values$Cluster_comp_detail <- ifelse(Joint_values$Var1_cluster == Joint_values$Var2_cluster,Joint_values$Var1_cluster,paste(Joint_values$Var1_cluster,"other",sep="_"))


save(df_poporder,melted_cormat_anno,melted_combined_anno_red,Joint_values,All_Sample_list,file = paste0(folder_path_data,"NEA_cor_vs_F3_plots.RData"))

```


### Check corelation of estimates for EarlyOoA

```{r echo=FALSE, eval=TRUE}

load(paste0(folder_path_data,"NEA_cor_vs_F3_plots.RData"))

# Perform one-sided t-tests comparing the specific mean to all others
test_samples <- unique(Joint_values$Var1_cluster[Joint_values$Var1_cluster != "EarlyOoA"])

Comp_cor <- sapply(test_samples, function(i) {
    c(t.test(Joint_values$NEAcor[Joint_values$Var1_cluster == "EarlyOoA"], Joint_values$NEAcor[Joint_values$Var1_cluster == i], alternative = "less")$p.value,i,mean(Joint_values$NEAcor[Joint_values$Var1_cluster == "EarlyOoA"]),mean(Joint_values$NEAcor[Joint_values$Var1_cluster == i]))
})
Comp_cor = data.frame(EarlyOoA_cor = Comp_cor[3,],pop = Comp_cor[2,],Other_pop_cor = Comp_cor[4,],p_value = Comp_cor[1,])
# Adjust for multiple testing (Holm correction)
Comp_cor$adjusted_p_values <- p.adjust(Comp_cor$p_value, method = "holm")



Joint_values %>% mutate(clstr_x = ifelse(Var1_cluster == "EarlyOoA" | Var2_cluster == "EarlyOoA" ,"EarlyOoA","Other")) %>%
  ggplot(.,aes(x=F3_AA,y=NEAcor,fill=clstr_x)) +
  geom_point() +
  geom_smooth(method = "lm",col = "#009999") +
  THEME +
  xlab("F3 on archaic admixture + X") +
  ylab("Segment correlation") +
  stat_cor(method = "spearman")

```

## Matching Analysis

Lastly we look at the matching of the segments to the reference archaic individuals. The data can be found in the Dryad repo.

```{r echo=FALSE, eval=TRUE, results='hide'}


All_rle_Matching_ascertainment <- read.csv(paste0(path_to_dryad_downloaded_files,"/Neandertal_segments_matching_references_Shared_map.csv")) %>% filter(Sites_used == "Matching_archaic_admixture") %>% mutate(sites = "all")


MatchingRates_file_MatchingAScerteinment_anno <- inner_join(All_rle_Matching_ascertainment,Joint_Meta[,c("sample_name","superpopulation_cluster",cluster_used,"Cov","ML_BP_Mean","pop","Sex","x_chrom_captured","DataType","ave_cont")],by=c("sample"="sample_name")) %>% 
  mutate(time_slice = ifelse(ML_BP_Mean >= 30000,30000,ifelse(ML_BP_Mean >= 15000,15000,ifelse(ML_BP_Mean >= 5000,5000,0)))) %>%
  mutate(time_period = ifelse(ML_BP_Mean >= 15000,"pre-LGM",ifelse(ML_BP_Mean > 0,"post-LGM","Present-Day"))) %>%
  mutate(Sites = "MatchingAscertainment")
MatchingRates_file_MatchingAScerteinment_anno$time_period <-  factor(MatchingRates_file_MatchingAScerteinment_anno$time_period , levels = c("pre-LGM", "post-LGM", "Present-Day"))


MatchingRates_file_MatchingAScerteinment_anno_non_overlapping_list = MatchingRates_file_MatchingAScerteinment_anno %>% group_overlaps_fn(.)  %>% 
  filter(chrom != "X", n_all_snps > 10) %>%
  group_by(!!sym(cluster_used),group_id) %>% 
  dplyr::summarize(sampled_frag = sample(x = frag_ID,size = 1,replace = T)) %>% ungroup() 
  
MatchingRates_file_MatchingAScerteinment_anno_non_overlapping <- inner_join(MatchingRates_file_MatchingAScerteinment_anno_non_overlapping_list[,c("sampled_frag")],MatchingRates_file_MatchingAScerteinment_anno,by=c("sampled_frag"="frag_ID"))

MatchingRates_file_MatchingAScerteinment_anno_non_overlapping <- MatchingRates_file_MatchingAScerteinment_anno_non_overlapping %>% filter(sample != "LeangPanninge") 


```

```{r echo=FALSE, eval=TRUE, results='hide'}


All_rle_OnlyVariable <- read.csv(paste0(path_to_dryad_downloaded_files,"/Neandertal_segments_matching_references_Shared_map.csv")) %>% filter(Sites_used == "All_diagnostic_sites") %>% mutate(sites = "all")


MatchingRates_file_OnlyVariable_anno <- inner_join(All_rle_OnlyVariable,Joint_Meta[,c("sample_name","superpopulation_cluster",cluster_used,"Cov","ML_BP_Mean","pop","Sex","x_chrom_captured","DataType","ave_cont")],by=c("sample"="sample_name")) %>% 
  mutate(time_slice = ifelse(ML_BP_Mean >= 30000,30000,ifelse(ML_BP_Mean >= 15000,15000,ifelse(ML_BP_Mean >= 5000,5000,0)))) %>%
  mutate(time_period = ifelse(ML_BP_Mean >= 15000,"pre-LGM",ifelse(ML_BP_Mean > 0,"post-LGM","Present-Day"))) %>%
  mutate(Sites = "OnlyVariable")
MatchingRates_file_OnlyVariable_anno$time_period <-  factor(MatchingRates_file_OnlyVariable_anno$time_period , levels = c("pre-LGM", "post-LGM", "Present-Day"))


MatchingRates_file_OnlyVariable_anno_non_overlapping_list = MatchingRates_file_OnlyVariable_anno %>% group_overlaps_fn(.)  %>% 
  filter(chrom != "X", n_all_snps > 10) %>%
  group_by(!!sym(cluster_used),group_id) %>% 
  dplyr::summarize(sampled_frag = sample(x = frag_ID,size = 1,replace = T)) %>% ungroup() 
  
MatchingRates_file_OnlyVariable_anno_non_overlapping <- inner_join(MatchingRates_file_OnlyVariable_anno_non_overlapping_list[,c("sampled_frag")],MatchingRates_file_OnlyVariable_anno,by=c("sampled_frag"="frag_ID"))

MatchingRates_file_OnlyVariable_anno_non_overlapping <- MatchingRates_file_OnlyVariable_anno_non_overlapping %>% filter(sample != "LeangPanninge") 


```

```{r echo=FALSE, eval=TRUE, results='hide'}
## Joint DF ##
MatchingRates_file_all_anno <- rbind(MatchingRates_file_MatchingAScerteinment_anno,MatchingRates_file_OnlyVariable_anno )

MatchingRates_file_all_anno_non_overlapping_list = MatchingRates_file_all_anno %>%
  filter(chrom != "X", n_all_snps > 10) %>%
  group_overlaps_fn(.)  %>% 
  group_by(!!sym(cluster_used),group_id) %>% 
  dplyr::summarize(sampled_frag = sample(x = frag_ID,size = 1,replace = T)) %>% ungroup() 
  
MatchingRates_file_anno_non_overlapping <- inner_join(MatchingRates_file_all_anno_non_overlapping_list[,c("sampled_frag")],MatchingRates_file_all_anno,by=c("sampled_frag"="frag_ID"))

save(MatchingRates_file_all_anno,file = paste0(folder_path_data,"MatchingRates_file_all_anno",genetic_map,".RData"))
save(MatchingRates_file_anno_non_overlapping,file = paste0(folder_path_data,"MatchingRates_file_all_anno_non_overlapping_",genetic_map,".RData"))
```

## Overall matching using proportion pseudo Haploid matching

The technical validation using Kostenki14 shotgun and captured revealed no difference in either taking proportion of reads matching derived sites or proportion pseudo Haploid matching. The proportions of either derived reads per total amount of reads or the number of derived alleles per covered sites are normalized for coverage (only covered sites are counted). However the exact matching rate to a given reference genome will be influenced by the type of data. Captured data are much more likely to capture the reference allele in the "bycatched" sites then the alternative one. Hence, the match rates to an reference sample can only be compared to using the same data. However the the difference in data only shifts all matches on the diagonal but not towards a certain reference. Therefore exact match rates can only be compared using the same data type (preferably shotgun) but the attraction to a given reference compared to another (2D plot everything going of the diagonal) is comparable. Plot scripts are in the joint_plots_paper script.

To execute this analysis you need to run the admixfrog pipeline for bith Kostenki14 data sets the data is not provided on the Dryad repo!.

### Technical Valdiation using Kostenki 14

#### Data & Visualization

```{r eval=T,echo=F,results='hide'}
## all
DiagSites = "all"
K14_shotgun <- get_rle_files(paste0("TestLineageAssignment_Shared_Map/Diagnostic_sites/error2/length_bin_15/5000/AFR-NEA-DEN/",genetic_map,"/",DiagSites,"/Kostenki14Shotgun_archaicadmixtureAPX.rle0.25.match_summary.xz"),"_",c("state"),states,chrom_) %>% 
   filter(map_len >= min_len_ancient, pos_len >= min_bp_len_ancient) %>% mutate(Data = "Shotgun",Cov = 2.52)

K14_capture <- get_rle_files(paste0("TestLineageAssignment_Shared_Map/Diagnostic_sites/error2/length_bin_15/5000/AFR-NEA-DEN/",genetic_map,"/",DiagSites,"/Kostenki14AA_archaicadmixtureAPX.rle0.25.match_summary.xz"),"_",c("state"),states,chrom_) %>% 
   filter(map_len >= min_len_ancient, pos_len >= min_bp_len_ancient) %>% mutate(Data = "Capture",Cov = 16.1)

Kostenki_Test_all <- rbind(K14_shotgun,K14_capture) %>% mutate(Sites = "all")

Kostenki_Test_class_all <- match_BT_fn(Kostenki_Test_all)

## OnlyVariable
DiagSites = "OnlyVariable"
K14_shotgun <- get_rle_files(paste0("TestLineageAssignment_Shared_Map/Diagnostic_sites/error2/length_bin_15/5000/AFR-NEA-DEN/",genetic_map,"/",DiagSites,"/Kostenki14Shotgun_archaicadmixtureAPX.rle0.25.match_summary.xz"),"_",c("state"),states,chrom_) %>% 
   filter(map_len >= min_len_ancient, pos_len >= min_bp_len_ancient) %>% mutate(Data = "Shotgun",Cov = 2.52)

K14_capture <- get_rle_files(paste0("TestLineageAssignment_Shared_Map/Diagnostic_sites/error2/length_bin_15/5000/AFR-NEA-DEN/",genetic_map,"/",DiagSites,"/Kostenki14AA_archaicadmixtureAPX.rle0.25.match_summary.xz"),"_",c("state"),states,chrom_) %>% 
   filter(map_len >= min_len_ancient, pos_len >= min_bp_len_ancient) %>% mutate(Data = "Capture",Cov = 16.1)

Kostenki_Test_OnlyVariable <- rbind(K14_shotgun,K14_capture) %>% mutate(Sites = "OnlyVariable")

Kostenki_Test_class_OnlyVariable <- match_BT_fn(Kostenki_Test_OnlyVariable)

## MatchingAscertainment
DiagSites = "MatchingAscertainment"
K14_shotgun <- get_rle_files(paste0("TestLineageAssignment_Shared_Map/Diagnostic_sites/error2/length_bin_15/5000/AFR-NEA-DEN/",genetic_map,"/",DiagSites,"/Kostenki14Shotgun_archaicadmixtureAPX.rle0.25.match_summary.xz"),"_",c("state"),states,chrom_) %>% 
   filter(map_len >= min_len_ancient, pos_len >= min_bp_len_ancient) %>% mutate(Data = "Shotgun",Cov = 2.52)

K14_capture <- get_rle_files(paste0("TestLineageAssignment_Shared_Map/Diagnostic_sites/error2/length_bin_15/5000/AFR-NEA-DEN/",genetic_map,"/",DiagSites,"/Kostenki14AA_archaicadmixtureAPX.rle0.25.match_summary.xz"),"_",c("state"),states,chrom_) %>% 
   filter(map_len >= min_len_ancient, pos_len >= min_bp_len_ancient) %>% mutate(Data = "Capture",Cov = 16.1)

Kostenki_Test_MatchingAscertainment <- rbind(K14_shotgun,K14_capture) %>% mutate(Sites = "MatchingAscertainment")

Kostenki_Test_class_MatchingAscertainment <- match_BT_fn(Kostenki_Test_MatchingAscertainment)

Kostenki_Test_matching_reads <- Kostenki_Test_class_OnlyVariable %>% dplyr::select(sample,target,chrom,n_all_snps,Data,Cov,prop_matching_Mbuti,prop_matching_Altai,prop_matching_Vindija33.19,prop_matching_Chagyrskaya,prop_matching_Denisova,Highest_matching_archaic) %>% 
  dplyr::rename(Mbuti = prop_matching_Mbuti,Altai = prop_matching_Altai,Vindija33.19 = prop_matching_Vindija33.19,Chagyrskaya = prop_matching_Chagyrskaya,Denisova = prop_matching_Denisova) %>%
  mutate(method = "reads")

Kostenki_Test_matching_pH <- Kostenki_Test_class_OnlyVariable %>% dplyr::select(sample,target,chrom,n_all_snps,Data,Cov,prop_pHcount_matching_Mbuti,prop_pHcount_matching_Altai,prop_pHcount_matching_Vindija33.19,prop_pHcount_matching_Chagyrskaya,prop_pHcount_matching_Denisova,Highest_matching_archaic) %>% 
    dplyr::rename(Mbuti = prop_pHcount_matching_Mbuti,Altai = prop_pHcount_matching_Altai,Vindija33.19 = prop_pHcount_matching_Vindija33.19,Chagyrskaya = prop_pHcount_matching_Chagyrskaya,Denisova = prop_pHcount_matching_Denisova) %>%
  mutate(method = "pseudohaploid")

Kostenki_Test_matching = rbind(Kostenki_Test_matching_reads,Kostenki_Test_matching_pH)

rm(Kostenki_Test_matching_reads,Kostenki_Test_matching_pH)

save(Kostenki_Test_matching, file = paste0(folder_path_data,"KostenkiTesting_OnlyVariable.RData"))

# Chi Square test fo independence

## check different estimates
Kostenki_Test_class_red = Kostenki_Test_matching %>% filter(Highest_matching_archaic %in% c("Altai","Chagyrskaya","Vindija33.19","Denisova"),Data == "Shotgun")

Kostenki_Test_class_table = as.data.frame(table(Kostenki_Test_class_red[,c("method","Highest_matching_archaic")])) %>% pivot_wider(names_from = Highest_matching_archaic, values_from = Freq) %>% column_to_rownames(.,var = "method")

chisq <- chisq.test(Kostenki_Test_class_table)
chisq

## check different sequencing method
Kostenki_Test_class_red = Kostenki_Test_matching %>% filter(Highest_matching_archaic %in% c("Altai","Chagyrskaya","Vindija33.19","Denisova"),method == "pseudohaploid")

Kostenki_Test_class_table = as.data.frame(table(Kostenki_Test_class_red[,c("sample","Highest_matching_archaic")])) %>% pivot_wider(names_from = Highest_matching_archaic, values_from = Freq) %>% column_to_rownames(.,var = "sample")

chisq <- chisq.test(Kostenki_Test_class_table)
chisq

```

#### Modeling Bias

We can model the bias using a Bayesian GLM as described in the manuscript.

```{r echo=FALSE, eval=TRUE, results='hide'}
Kostenki_Test_class_model <- Kostenki_Test_matching %>% dplyr::select(sample,method,Highest_matching_archaic,Cov) %>% filter(Highest_matching_archaic %in% c("Altai","Chagyrskaya","Vindija33.19","Denisova"))

### Poisson model ###

Kostenki_Test_class_model_table <- Kostenki_Test_class_model %>%
  mutate(Highest_matching_archaic = ifelse(Highest_matching_archaic == "Vindija33.19","Vindija",Highest_matching_archaic)) %>%
  group_by(Highest_matching_archaic,sample,method) %>% summarise(count = n()) %>%
  pivot_wider(names_from = Highest_matching_archaic, values_from = count) %>% replace(is.na(.), 0)

dat <- list(datatype =  as.numeric(as.factor(Kostenki_Test_class_model_table$sample)),
            Altai = as.numeric(Kostenki_Test_class_model_table$Altai),
            Chagyrskaya = as.numeric(Kostenki_Test_class_model_table$Chagyrskaya),
            Denisova = as.numeric(Kostenki_Test_class_model_table$Denisova),
            Vindija = as.numeric(Kostenki_Test_class_model_table$Vindija),
            method=as.numeric(as.factor(Kostenki_Test_class_model_table$method)) -1)

m_pois_realdate <- ulam(
  alist(
    Altai ~ dpois(lambda1),
    Chagyrskaya ~ dpois(lambda2),
    Denisova ~ dpois(lambda3),
    Vindija ~ dpois(lambda4),
    log(lambda1) <- a_1[datatype] + b_method1*method,
    log(lambda2) <- a_2[datatype] + b_method2*method,
    log(lambda3) <- a_3[datatype] + b_method3*method,
    log(lambda4) <- a_4[datatype] + b_method4*method,
    a_1[datatype] ~ dnorm(0,1.5),
    a_2[datatype] ~ dnorm(0,1.5),
    a_3[datatype] ~ dnorm(0,1.5),
    a_4[datatype] ~ dnorm(0,1.5),
    c(b_method1,b_method2,b_method3,b_method4) ~ dnorm(0,2)
  ), data=dat , chains=4 , cores=4 )


dat <- list(datatype =  as.numeric(as.factor(Kostenki_Test_class_model_table$sample)) -1,
            Altai = as.numeric(Kostenki_Test_class_model_table$Altai),
            Chagyrskaya = as.numeric(Kostenki_Test_class_model_table$Chagyrskaya),
            Denisova = as.numeric(Kostenki_Test_class_model_table$Denisova),
            Vindija = as.numeric(Kostenki_Test_class_model_table$Vindija),
            method=as.numeric(as.factor(Kostenki_Test_class_model_table$method)) -1)

m_pois1_realdate <- ulam(
  alist(
    Altai ~ dpois(lambda1),
    Chagyrskaya ~ dpois(lambda2),
    Denisova ~ dpois(lambda3),
    Vindija ~ dpois(lambda4),
    log(lambda1) <- a_1 + b_dtype1*datatype + b_method1*method,
    log(lambda2) <- a_2 + b_dtype2*datatype + b_method2*method,
    log(lambda3) <- a_3 + b_dtype3*datatype + b_method3*method,
    log(lambda4) <- a_4 + b_dtype4*datatype + b_method4*method,
    c(a_1,a_2,a_3,a_4) ~ dnorm(0,1.5),
    c(b_method1,b_method2,b_method3,b_method4) ~ dnorm(0,2),
    c(b_dtype1,b_dtype2,b_dtype3,b_dtype4) ~ dnorm(0,2)
  ), data=dat , chains=4 , cores=4 )

plot(precis(m_pois_realdate,3))
plot(precis(m_pois1_realdate,3))

post <- extract.samples(m_pois_realdate)

pop_intercepts_pois_ri <- list()
for(grp in 1:length(unique(Kostenki_Test_class_model_table$sample))){
  pop_intercepts_pois_ri[[grp]] <-   matrix(sapply(1:length(post$a_1[,1]), function(i) c(
    exp(post$a_1[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
    exp(post$a_2[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
    exp(post$a_3[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
    exp(post$a_4[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])))),ncol = 4,byrow = T)

}

p_1_pois_ri = mean(pop_intercepts_pois_ri[[1]][,1])
p_2_pois_ri = mean(pop_intercepts_pois_ri[[1]][,2])
p_3_pois_ri = mean(pop_intercepts_pois_ri[[1]][,3])
p_4_pois_ri = mean(pop_intercepts_pois_ri[[1]][,4])

post <- extract.samples(m_pois1_realdate)


pop_intercepts_pois <-   data.frame(
  p1 = exp(post$a_1)/(exp(post$a_1)+exp(post$a_2)+exp(post$a_3)+exp(post$a_4)),
  p2 = exp(post$a_2)/(exp(post$a_1)+exp(post$a_2)+exp(post$a_3)+exp(post$a_4)),
  p3 = exp(post$a_3)/(exp(post$a_1)+exp(post$a_2)+exp(post$a_3)+exp(post$a_4)),
  p4 = exp(post$a_4)/(exp(post$a_1)+exp(post$a_2)+exp(post$a_3)+exp(post$a_4)))


p_1_pois = mean(pop_intercepts_pois$p1)
p_2_pois = mean(pop_intercepts_pois$p2)
p_3_pois = mean(pop_intercepts_pois$p3)
p_4_pois = mean(pop_intercepts_pois$p4)



effect_method <-   as.data.frame(t(sapply(1:length(post$b_method1), function(i) c(
  exp(post$b_method1[i])/(exp(post$b_method1[i])+exp(post$b_method2[i])+exp(post$b_method3[i])+exp(post$b_method4[i])),
  exp(post$b_method2[i])/(exp(post$b_method1[i])+exp(post$b_method2[i])+exp(post$b_method3[i])+exp(post$b_method4[i])),
  exp(post$b_method3[i])/(exp(post$b_method1[i])+exp(post$b_method2[i])+exp(post$b_method3[i])+exp(post$b_method4[i])),
  exp(post$b_method4[i])/(exp(post$b_method1[i])+exp(post$b_method2[i])+exp(post$b_method3[i])+exp(post$b_method4[i])))))) %>%
  mutate(change_ALT = V1 - V4,change_CHA = V2 - V4,change_DEN = V3 - V4) %>% dplyr::select(change_ALT,change_CHA,change_DEN)

effect_method_summary <- effect_method %>% dplyr:: summarize(mean_change_ALT = mean(change_ALT),
                                      sd_change_ALT = sd(change_ALT),
                                      lower_HDPI_change_ALT = HPDI(change_ALT, 0.95)[1],
                                      upper_HDPI_change_ALT = HPDI(change_ALT, 0.95)[2],
                                      mean_change_CHA = mean(change_CHA),
                                      sd_change_CHA = sd(change_CHA),
                                      lower_HDPI_change_CHA = HPDI(change_CHA, 0.95)[1],
                                      upper_HDPI_change_CHA = HPDI(change_CHA, 0.95)[2],
                                      mean_change_DEN = mean(change_DEN),
                                      sd_change_DEN = sd(change_DEN),
                                      lower_HDPI_change_DEN = HPDI(change_DEN, 0.95)[1],
                                      upper_HDPI_change_DEN = HPDI(change_DEN, 0.95)[2])

effect_datatype <-   as.data.frame(t(sapply(1:length(post$b_dtype1), function(i) c(
  exp(post$b_dtype1[i])/(exp(post$b_dtype1[i])+exp(post$b_dtype2[i])+exp(post$b_dtype3[i])+exp(post$b_dtype4[i])),
  exp(post$b_dtype2[i])/(exp(post$b_dtype1[i])+exp(post$b_dtype2[i])+exp(post$b_dtype3[i])+exp(post$b_dtype4[i])),
  exp(post$b_dtype3[i])/(exp(post$b_dtype1[i])+exp(post$b_dtype2[i])+exp(post$b_dtype3[i])+exp(post$b_dtype4[i])),
  exp(post$b_dtype4[i])/(exp(post$b_dtype1[i])+exp(post$b_dtype2[i])+exp(post$b_dtype3[i])+exp(post$b_dtype4[i])))))) %>%
  mutate(change_ALT = V1 - V4,change_CHA = V2 - V4,change_DEN = V3 - V4) %>% dplyr::select(change_ALT,change_CHA,change_DEN)

effect_datatype_summary <- effect_datatype %>% dplyr:: summarize(mean_change_ALT = mean(change_ALT),
                                      sd_change_ALT = sd(change_ALT),
                                      lower_HDPI_change_ALT = HPDI(change_ALT, 0.95)[1],
                                      upper_HDPI_change_ALT = HPDI(change_ALT, 0.95)[2],
                                      mean_change_CHA = mean(change_CHA),
                                      sd_change_CHA = sd(change_CHA),
                                      lower_HDPI_change_CHA = HPDI(change_CHA, 0.95)[1],
                                      upper_HDPI_change_CHA = HPDI(change_CHA, 0.95)[2],
                                      mean_change_DEN = mean(change_DEN),
                                      sd_change_DEN = sd(change_DEN),
                                      lower_HDPI_change_DEN = HPDI(change_DEN, 0.95)[1],
                                      upper_HDPI_change_DEN = HPDI(change_DEN, 0.95)[2])

Plot_diff_seg_assignment_DT_pois <- effect_datatype %>% 
  pivot_longer(c(change_ALT,change_CHA,change_DEN), names_to = "Source", values_to = "prop") %>%
  ggplot(.,aes(x=prop,col=Source)) +
    geom_density() + 
    geom_vline(xintercept = 0,linetype = "dashed") +
    ggtitle("Effect of data type")


Plot_diff_seg_assignment_method_pois <- effect_method %>% 
  pivot_longer(c(change_ALT,change_CHA,change_DEN), names_to = "Source", values_to = "prop") %>%
  ggplot(.,aes(x=prop,col=Source)) +
    geom_density() + 
    geom_vline(xintercept = 0,linetype = "dashed") +
    ggtitle("Effect of estimation method")

save(pop_intercepts_pois,effect_datatype,effect_method, file = paste0(folder_path_data,"KostenkiTestingModeling_OnlyVariable.RData"))
```

### Autosomes

Next we can check the overlall assignment of the segment in an individual to an archaic reference.

##### by whole individual

Not very informative all are closest to Vindija except Leang Panninge (very low coverage) and two post-LGM WEur individuals but the matching of Vindija and Chagyrskaya is almost the same.

```{r echo=FALSE, eval=TRUE, results='hide'}

min_n_ref = 1

MatchingRates_file_anno_class_ind <- MatchingRates_file_all_anno  %>% filter(target == "NEA") %>% filter(chrom != "X", n_all_snps > 10) %>% 
  group_by(sample,superpopulation_cluster,(!!sym(cluster_used)),time_period,target,Sites) %>% summarise(
              all_prop_matching_Mbuti = sum(count_matching_Mbuti,na.rm = T)/sum(count_matching_Mbuti/prop_pHcount_matching_Mbuti,na.rm = T),
              all_prop_matching_Vindija33.19 = sum(count_matching_Vindija33.19,na.rm = T)/sum(count_matching_Vindija33.19/prop_pHcount_matching_Vindija33.19,na.rm = T),
              all_prop_matching_Altai = sum(count_matching_Altai,na.rm = T)/sum(count_matching_Altai/prop_pHcount_matching_Altai,na.rm = T),
              all_prop_matching_Chagyrskaya = sum(count_matching_Chagyrskaya,na.rm = T)/sum(count_matching_Chagyrskaya/prop_pHcount_matching_Chagyrskaya,na.rm = T),
              all_prop_matching_Denisova = sum(count_matching_Denisova,na.rm = T)/sum(count_matching_Denisova/prop_pHcount_matching_Denisova,na.rm = T)) %>% ungroup() %>%
    filter(!if_all(all_of(c("all_prop_matching_Vindija33.19","all_prop_matching_Altai","all_prop_matching_Chagyrskaya","all_prop_matching_Denisova")), is.na))


Highest_matching_archaic = MatchingRates_file_anno_class_ind %>% dplyr::select(all_prop_matching_Vindija33.19,all_prop_matching_Altai,all_prop_matching_Chagyrskaya,all_prop_matching_Denisova) %>%
  replace(is.na(.), 0) %>% dplyr::rename(Vindija33.19 = all_prop_matching_Vindija33.19,
                                         Altai = all_prop_matching_Altai,
                                         Chagyrskaya = all_prop_matching_Chagyrskaya,
                                         Denisova = all_prop_matching_Denisova) %>% apply(.,1,function(x) paste(sort(names(which(x==max(x)))),collapse = ", "))

Highest_matching_archaic_value = MatchingRates_file_anno_class_ind %>% dplyr::select(all_prop_matching_Vindija33.19,all_prop_matching_Altai,all_prop_matching_Chagyrskaya,all_prop_matching_Denisova) %>%
  replace(is.na(.), 0) %>%  mutate(Highest_matching_archaic_value = pmax(all_prop_matching_Vindija33.19,all_prop_matching_Altai,all_prop_matching_Chagyrskaya,all_prop_matching_Denisova))


MatchingRates_file_anno_class_ind$Highest_matching_archaic <- Highest_matching_archaic
MatchingRates_file_anno_class_ind$Highest_matching_archaic_value <- Highest_matching_archaic_value$Highest_matching_archaic_value

rm(Highest_matching_archaic)


Tip_branch_matching_whole_ind_Ppop_non_overlapping <- MatchingRates_file_anno_class_ind %>%
  filter(Sites == "MatchingAscertainment") %>%
  ggplot(.,aes(x = factor(1),fill=factor(Highest_matching_archaic))) + 
    facet_wrap(time_period~get(cluster_used),) +
    geom_bar(width = 1,position = "fill") + 
    coord_polar(theta="y") +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          panel.grid  = element_blank()) +
    guides(fill=guide_legend(title="Highest matching:"))

Tip_branch_matching_whole_ind_Ppop_non_overlapping <- MatchingRates_file_anno_class_ind %>%
  filter(Sites == "OnlyVariable") %>%
  ggplot(.,aes(x = factor(1),fill=factor(Highest_matching_archaic))) + 
    facet_wrap(time_period~get(cluster_used),) +
    geom_bar(width = 1,position = "fill") + 
    coord_polar(theta="y") +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          panel.grid  = element_blank()) +
    guides(fill=guide_legend(title="Highest matching:"))


```

##### by individual segment

More informative is looking at the matching of each segments.

```{r echo=FALSE, eval=TRUE, results='hide'}

MatchingRates_file_anno_class <- MatchingRates_file_all_anno %>% filter(target == "NEA") %>% filter(chrom != "X", n_all_snps > 10) %>%
    filter(!if_all(all_of(c("prop_pHcount_matching_Vindija33.19","prop_pHcount_matching_Altai","prop_pHcount_matching_Chagyrskaya","prop_pHcount_matching_Denisova")), is.na))

Highest_matching_archaic = MatchingRates_file_anno_class %>% dplyr::select(prop_pHcount_matching_Vindija33.19,prop_pHcount_matching_Altai,prop_pHcount_matching_Chagyrskaya,prop_pHcount_matching_Denisova) %>%
  replace(is.na(.), 0) %>% dplyr::rename(Vindija33.19 = prop_pHcount_matching_Vindija33.19,
                                         Altai = prop_pHcount_matching_Altai,
                                         Chagyrskaya = prop_pHcount_matching_Chagyrskaya,
                                         Denisova = prop_pHcount_matching_Denisova) %>% apply(.,1,function(x) paste(sort(names(which(x==max(x)))),collapse = ", "))

Highest_matching_archaic_value = MatchingRates_file_anno_class %>% dplyr::select(prop_pHcount_matching_Vindija33.19,prop_pHcount_matching_Altai,prop_pHcount_matching_Chagyrskaya,prop_pHcount_matching_Denisova) %>%
  replace(is.na(.), 0) %>%  mutate(Highest_matching_archaic_value = pmax(prop_pHcount_matching_Vindija33.19,prop_pHcount_matching_Altai,prop_pHcount_matching_Chagyrskaya,prop_pHcount_matching_Denisova))

MatchingRates_file_anno_class$Highest_matching_archaic <- Highest_matching_archaic
MatchingRates_file_anno_class$Highest_matching_archaic_value <- Highest_matching_archaic_value$Highest_matching_archaic_value

MatchingRates_file_anno_class <- MatchingRates_file_anno_class %>% mutate(Highest_matching_archaic = ifelse(Highest_matching_archaic_value == 0,NA,Highest_matching_archaic))

MatchingRates_file_anno_class = MatchingRates_file_anno_class %>% mutate(tree = ifelse((Highest_matching_archaic == "Altai, Chagyrskaya, Vindija33.19" | Highest_matching_archaic == "Chagyrskaya, Vindija33.19" | Highest_matching_archaic == "Altai, Chagyrskaya, Denisova, Vindija33.19" | 
                                                                                          Highest_matching_archaic == "Altai" |
                                                                                          Highest_matching_archaic == "Chagyrskaya" |
                                                                                          Highest_matching_archaic == "Denisova" |
                                                                                          Highest_matching_archaic == "Vindija33.19" ),"cordant","discordant"))

MatchingRates_file_anno_class_non_overlapping_list = MatchingRates_file_anno_class %>%
  filter(chrom != "X", n_all_snps > 10) %>%
  group_overlaps_fn(.)  %>% 
  group_by(!!sym(cluster_used),group_id) %>% 
  dplyr::summarize(sampled_frag = sample(x = frag_ID,size = 1,replace = T)) %>% ungroup() 
  
MatchingRates_file_anno_class_non_overlapping <- inner_join(MatchingRates_file_anno_class_non_overlapping_list[,c("sampled_frag")],MatchingRates_file_anno_class,by=c("sampled_frag"="frag_ID"))

save(MatchingRates_file_anno_class,file = paste0(folder_path_data,"MatchingRates_file_all_anno_class_",genetic_map,".RData"))

save(MatchingRates_file_anno_class_non_overlapping,file = paste0(folder_path_data,"MatchingRates_file_anno_class_non_overlapping_",genetic_map,".RData"))
```

###### using bootstrap values

```{r echo=FALSE, eval=TRUE, results='hide'}

MatchingRates_file_anno_class_BT <- MatchingRates_file_anno_class %>% filter(target == "NEA") %>% filter(chrom != "X", n_all_snps > 10) %>%
    filter(!if_all(all_of(c("BT_prop_pHcount_matching_Vindija33.19_mean","BT_prop_pHcount_matching_Altai_mean","BT_prop_pHcount_matching_Chagyrskaya_mean","BT_prop_pHcount_matching_Denisova_mean")), is.na))

Highest_matching_archaic = MatchingRates_file_anno_class_BT %>% dplyr::select(BT_prop_pHcount_matching_Vindija33.19_mean,BT_prop_pHcount_matching_Altai_mean,BT_prop_pHcount_matching_Chagyrskaya_mean,BT_prop_pHcount_matching_Denisova_mean) %>%
  replace(is.na(.), 0) %>% dplyr::rename(Vindija33.19 = BT_prop_pHcount_matching_Vindija33.19_mean,
                                         Altai = BT_prop_pHcount_matching_Altai_mean,
                                         Chagyrskaya = BT_prop_pHcount_matching_Chagyrskaya_mean,
                                         Denisova = BT_prop_pHcount_matching_Denisova_mean) %>% apply(.,1,function(x) paste(sort(names(which(x==max(x)))),collapse = ", "))

Highest_matching_archaic_value = MatchingRates_file_anno_class_BT %>% dplyr::select(BT_prop_pHcount_matching_Vindija33.19_mean,BT_prop_pHcount_matching_Altai_mean,BT_prop_pHcount_matching_Chagyrskaya_mean,BT_prop_pHcount_matching_Denisova_mean) %>%
  replace(is.na(.), 0) %>%  mutate(Highest_matching_archaic_value = pmax(BT_prop_pHcount_matching_Vindija33.19_mean,BT_prop_pHcount_matching_Altai_mean,BT_prop_pHcount_matching_Chagyrskaya_mean,BT_prop_pHcount_matching_Denisova_mean))

MatchingRates_file_anno_class_BT$Highest_matching_archaic <- Highest_matching_archaic
MatchingRates_file_anno_class_BT$Highest_matching_archaic_value <- Highest_matching_archaic_value$Highest_matching_archaic_value

MatchingRates_file_anno_class_BT <- MatchingRates_file_anno_class_BT %>% mutate(Highest_matching_archaic = ifelse(Highest_matching_archaic_value == 0,NA,Highest_matching_archaic))

MatchingRates_file_anno_class_BT = MatchingRates_file_anno_class_BT %>% mutate(tree = ifelse((Highest_matching_archaic == "Altai, Chagyrskaya, Vindija33.19" | Highest_matching_archaic == "Chagyrskaya, Vindija33.19" | Highest_matching_archaic == "Altai, Chagyrskaya, Denisova, Vindija33.19" | 
                                                                                          Highest_matching_archaic == "Altai" |
                                                                                          Highest_matching_archaic == "Chagyrskaya" |
                                                                                          Highest_matching_archaic == "Denisova" |
                                                                                          Highest_matching_archaic == "Vindija33.19" ),"cordant","discordant"))

MatchingRates_file_anno_class_BT_non_overlapping_list = MatchingRates_file_anno_class_BT %>%
  filter(chrom != "X", n_all_snps > 10) %>%
  group_overlaps_fn(.)  %>% 
  group_by(!!sym(cluster_used),group_id) %>% 
  dplyr::summarize(sampled_frag = sample(x = frag_ID,size = 1,replace = T)) %>% ungroup() 
  
MatchingRates_file_anno_class_BT_non_overlapping <- inner_join(MatchingRates_file_anno_class_BT_non_overlapping_list[,c("sampled_frag")],MatchingRates_file_anno_class_BT,by=c("sampled_frag"="frag_ID"))

save(MatchingRates_file_anno_class_BT,file = paste0(folder_path_data,"MatchingRates_file_anno_class_BT_",genetic_map,".RData"))

save(MatchingRates_file_anno_class_BT_non_overlapping,file = paste0(folder_path_data,"MatchingRates_file_all_anno_class_BT_non_overlapping_",genetic_map,".RData"))
```

##### Chi square test of independence between segment matching and population variables

```{r echo=FALSE, eval=TRUE, results='hide'}
# SUPERPOPULATIONS
MatchingRates_file_anno_class_table_non_overlapping = MatchingRates_file_anno_class_non_overlapping  %>% filter(Highest_matching_archaic %in% c("Altai", "Chagyrskaya", "Denisova", "Vindija33.19")) %>% dplyr::select(superpopulation_cluster,Highest_matching_archaic)

MatchingRates_file_anno_class_table_non_overlapping = as.data.frame(table(MatchingRates_file_anno_class_table_non_overlapping)) %>% pivot_wider(names_from = Highest_matching_archaic, values_from = Freq) %>% column_to_rownames(.,var = "superpopulation_cluster") %>% dplyr::select(!Denisova)

chisq <- chisq.test(MatchingRates_file_anno_class_table_non_overlapping)
chisq
chisq$expected

round(chisq$residuals, 3)

contrib <- 100*chisq$residuals^2/chisq$statistic
round(contrib, 3)
```

```{r echo=FALSE, eval=TRUE, results='hide'}
# POPULATIONS
MatchingRates_file_anno_class_table_non_overlapping = MatchingRates_file_anno_class  %>% filter(Highest_matching_archaic %in% c("Altai", "Chagyrskaya", "Denisova", "Vindija33.19")) %>% dplyr::select((!!sym(cluster_used)),Highest_matching_archaic)

MatchingRates_file_anno_class_table_non_overlapping = as.data.frame(table(MatchingRates_file_anno_class_table_non_overlapping)) %>% pivot_wider(names_from = Highest_matching_archaic, values_from = Freq) %>% column_to_rownames(.,var = cluster_used) %>% dplyr::select(!Denisova)

chisq <- chisq.test(MatchingRates_file_anno_class_table_non_overlapping)
chisq
chisq$expected

round(chisq$residuals, 3)

contrib <- 100*chisq$residuals^2/chisq$statistic
round(contrib, 3)
```


##### Multinomial as Poisson model

We can build a Bayesian GLM to check if the frequency of matching to a reference archaic individual is different between population clusters while taking into account some potential confounders. The model is described in the manuscripts.

###### Simulations

First we test the model using simulated data.

```{r echo=FALSE, eval=TRUE, results='hide'}
library(rethinking)

sim_fun <- function(Ind,n_groups,N,anc_score,Group_modulate,Dtype_modulate,Cov_modulate,Age_modulate){
  # coverage per individual
  Ind = sample(1:n_Ind,size = N,prob = rep(1/n_Ind,n_Ind),replace = T)
  # segments belonging to introgressed group
  Seg = Ind
  Seg_group = sample(1:n_groups,size = n_Ind,prob = rep(1/n_groups,n_groups),replace = T)
  for(i in unique(Ind)){
    Seg[Ind==i] <- Seg_group[i]
  }

  # segments belonging to data type group
  Dtype = Ind 
  for(i in unique(Ind)){
    Dtype[Ind==i] <- rbinom(1,1,prob = groups_dt[Seg_group[i]])
  }
  
  
  # segments belonging to ind with certain coverage
  Cov = Ind 
  for(i in unique(Ind)){
    Cov[Ind==i] <- rgamma(1,groups_cov[[Seg_group[i]]][1],groups_cov[[Seg_group[i]]][2])
  }

  # segments belonging to ind with certain age
  Age = Ind 
  for(i in unique(Ind)){
    Age[Ind==i] <- rgamma(1,groups_age[[Seg_group[i]]][1],groups_age[[Seg_group[i]]][2])
  }

  # ration of ancestries for ALT, CHA and VIN
  ancestry <- rep(NA,N) # empty vector of ancestry of segments
  for(i in 1:length(Ind)){
    score = anc_score + Group_modulate[[Seg[i]]]  + Dtype_modulate * Dtype[i] + Cov_modulate * rethinking::normalize(Cov) + Age_modulate * rethinking::normalize(Age)
    p <- softmax(score[1],score[2],score[3],score[4])
    ancestry[i] <- sample(1:4, size = 1,prob = p)
  }
  
  sim_data <-  data.frame(
    Indx = Ind,
    Covx = Cov,
    Dtypex = Dtype,
    Segx = Seg,
    ancestryx = ancestry,
    Time = Age
    
  ) 
  
  sim_data_res = sim_data %>% group_by(Segx) %>% summarise(ave_Cov = mean(Covx),ave_age = mean(Time),DT = sum(Dtypex)/n())
  print(paste0("Simulated groups as follows:"))
  print(sim_data_res)
  
  return(sim_data)
}

# number of individuals
n_Ind = 80
# number population groups
n_groups = 4
# ave group DT
groups_dt = c(0.9,0,0.3,0.1)
# ave group cov
groups_cov = list(c(2,1),c(30,1),c(4,0.5),c(5,1))
# number population groups
groups_age = list(c(1000,1),c(1,100),c(400,1),c(200,1))
# number of all segments
N <- 10000
# ground probability of ancestry (if all the same number all ancestries are equally likely)
anc_score = c(0,0,0,0)
# differences of ancestry in groups itself
Group_modulate = list(c(0,0,0,0),
                 c(0,0,0,0),
                 c(0,0,0,0),
                 c(0,0,0,0))
# differences of ancestry due to data type
Dtype_modulate =  c(0,0,0,0)
# differences of ancestry due to coverage
Cov_modulate =  c(0,0,0,0)
# differences of ancestry due to sample age
Age_modulate =  c(0,0,0,0)

No_differnces <- sim_fun(Ind,n_groups,N,anc_score,Group_modulate,Dtype_modulate,Cov_modulate,Age_modulate)

# differences of ancestry in groups itself
Group_modulate = list(c(0.5,0,0,0),
                 c(0,0,0,0),
                 c(0,0,0,0.8),
                 c(0,0.5,0,0))

Differnces_in_groups <- sim_fun(Ind,n_groups,N,anc_score,Group_modulate,Dtype_modulate,Cov_modulate,Age_modulate)

# differences of ancestry in groups itself
Group_modulate = list(c(0,0,0,0),
                 c(0,0,0,0),
                 c(0,0,0,0),
                 c(0,0,0,0))
# differences of ancestry due to data type
Dtype_modulate =  c(2,0,0,0)

Differnces_in_DT <- sim_fun(Ind,n_groups,N,anc_score,Group_modulate,Dtype_modulate,Cov_modulate,Age_modulate)

# differences of ancestry due to coverage
Cov_modulate =  c(0,0,0,1)


Differnces_in_Cov_DT <- sim_fun(Ind,n_groups,N,anc_score,Group_modulate,Dtype_modulate,Cov_modulate,Age_modulate)

# differences of ancestry due to sample age
Age_modulate =  c(2,0,0,0)

Differnces_in_Cov_DT_T <- sim_fun(Ind,n_groups,N,anc_score,Group_modulate,Dtype_modulate,Cov_modulate,Age_modulate)


```

```{r echo=FALSE, eval=TRUE, results='hide'}
### Poisson model ###
run_model0 <- function(data){
  sim_data_table <- data %>%
  mutate(ancestryx = ifelse(ancestryx == 1,"Altai",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 2,"Chagyrskaya",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 3,"Denisova",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 4,"Vindija",ancestryx)) %>%
  group_by(Dtypex,Time,Covx,ancestryx,Segx) %>% summarise(count = n()) %>%
  pivot_wider(names_from = ancestryx, values_from = count) %>% replace(is.na(.), 0)


    
    
  dat <- list(popul =  as.numeric(as.factor(sim_data_table$Segx)),
              Altai = as.numeric(sim_data_table$Altai),
              Chagyrskaya = as.numeric(sim_data_table$Chagyrskaya),
              Denisova = as.numeric(sim_data_table$Denisova),
              Vindija = as.numeric(sim_data_table$Vindija),
              Cov=rethinking::standardize(as.numeric(sim_data_table$Covx)) ,
              Time=rethinking::standardize(as.numeric(sim_data_table$Time)) ,
              Dtype = as.numeric(as.factor(sim_data_table$Dtypex)) -1)
  
  m_pois0 <- ulam(
    alist(
      Altai ~ dpois(lambda1),
      Chagyrskaya ~ dpois(lambda2),
      Denisova ~ dpois(lambda3),
      Vindija ~ dpois(lambda4),
      log(lambda1) <- a_1[popul],
      log(lambda2) <- a_2[popul],
      log(lambda3) <- a_3[popul],
      log(lambda4) <- a_4[popul],
      a_1[popul] ~ dnorm(0,1.5),
      a_2[popul] ~ dnorm(0,1.5),
      a_3[popul] ~ dnorm(0,1.5),
      a_4[popul] ~ dnorm(0,1.5)
    ), data=dat , chains=4 , cores=4 ,log_lik = T)
  return(m_pois0)
}

run_model1 <- function(data){
  sim_data_table <- data %>%
  mutate(ancestryx = ifelse(ancestryx == 1,"Altai",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 2,"Chagyrskaya",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 3,"Denisova",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 4,"Vindija",ancestryx)) %>%
  group_by(Dtypex,Time,Covx,ancestryx,Segx) %>% summarise(count = n()) %>%
  pivot_wider(names_from = ancestryx, values_from = count) %>% replace(is.na(.), 0)


    
    
  dat <- list(popul =  as.numeric(as.factor(sim_data_table$Segx)),
              Altai = as.numeric(sim_data_table$Altai),
              Chagyrskaya = as.numeric(sim_data_table$Chagyrskaya),
              Denisova = as.numeric(sim_data_table$Denisova),
              Vindija = as.numeric(sim_data_table$Vindija),
              Cov=rethinking::standardize(as.numeric(sim_data_table$Covx)) ,
              Time=rethinking::standardize(as.numeric(sim_data_table$Time)) ,
              Dtype = as.numeric(as.factor(sim_data_table$Dtypex)) -1)
  
  m_pois0 <- ulam(
    alist(
      Altai ~ dpois(lambda1),
      Chagyrskaya ~ dpois(lambda2),
      Denisova ~ dpois(lambda3),
      Vindija ~ dpois(lambda4),
      log(lambda1) <- a_1[popul] + b_dtype1*Dtype,
      log(lambda2) <- a_2[popul] + b_dtype2*Dtype,
      log(lambda3) <- a_3[popul] + b_dtype3*Dtype,
      log(lambda4) <- a_4[popul] + b_dtype4*Dtype,
      a_1[popul] ~ dnorm(0,1.5),
      a_2[popul] ~ dnorm(0,1.5),
      a_3[popul] ~ dnorm(0,1.5),
      a_4[popul] ~ dnorm(0,1.5),
      c(b_dtype1,b_dtype2,b_dtype3,b_dtype4) ~ dnorm(0,1)
    ), data=dat , chains=4 , cores=4 ,log_lik = T)
  return(m_pois0)
}

run_model2 <- function(data){
  sim_data_table <- data %>%
  mutate(ancestryx = ifelse(ancestryx == 1,"Altai",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 2,"Chagyrskaya",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 3,"Denisova",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 4,"Vindija",ancestryx)) %>%
  group_by(Dtypex,Time,Covx,ancestryx,Segx) %>% summarise(count = n()) %>%
  pivot_wider(names_from = ancestryx, values_from = count) %>% replace(is.na(.), 0)


    
    
  dat <- list(popul =  as.numeric(as.factor(sim_data_table$Segx)),
              Altai = as.numeric(sim_data_table$Altai),
              Chagyrskaya = as.numeric(sim_data_table$Chagyrskaya),
              Denisova = as.numeric(sim_data_table$Denisova),
              Vindija = as.numeric(sim_data_table$Vindija),
              Cov=rethinking::standardize(as.numeric(sim_data_table$Covx)) ,
              Time=rethinking::standardize(as.numeric(sim_data_table$Time)) ,
              Dtype = as.numeric(as.factor(sim_data_table$Dtypex)) -1)
  
  m_pois2 <- ulam(
    alist(
      Altai ~ dpois(lambda1),
      Chagyrskaya ~ dpois(lambda2),
      Denisova ~ dpois(lambda3),
      Vindija ~ dpois(lambda4),
      log(lambda1) <- a_1[popul] + b_cov1*Cov + b_dtype1*Dtype,
      log(lambda2) <- a_2[popul] + b_cov2*Cov + b_dtype2*Dtype,
      log(lambda3) <- a_3[popul] + b_cov3*Cov + b_dtype3*Dtype,
      log(lambda4) <- a_4[popul] + b_cov4*Cov + b_dtype4*Dtype,
      a_1[popul] ~ dnorm(0,1.5),
      a_2[popul] ~ dnorm(0,1.5),
      a_3[popul] ~ dnorm(0,1.5),
      a_4[popul] ~ dnorm(0,1.5),
      c(b_cov1,b_cov2,b_cov3,b_cov4) ~ dnorm(0,1),
      c(b_dtype1,b_dtype2,b_dtype3,b_dtype4) ~ dnorm(0,1)
    ), data=dat , chains=4 , cores=4 ,log_lik = T)
  return(m_pois2)
}

run_model3 <- function(data){
  sim_data_table <- data %>%
  mutate(ancestryx = ifelse(ancestryx == 1,"Altai",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 2,"Chagyrskaya",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 3,"Denisova",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 4,"Vindija",ancestryx)) %>%
  group_by(Dtypex,Time,Covx,ancestryx,Segx) %>% summarise(count = n()) %>%
  pivot_wider(names_from = ancestryx, values_from = count) %>% replace(is.na(.), 0)


    
    
  dat <- list(popul =  as.numeric(as.factor(sim_data_table$Segx)),
              Altai = as.numeric(sim_data_table$Altai),
              Chagyrskaya = as.numeric(sim_data_table$Chagyrskaya),
              Denisova = as.numeric(sim_data_table$Denisova),
              Vindija = as.numeric(sim_data_table$Vindija),
              Cov=rethinking::standardize(as.numeric(sim_data_table$Covx)) ,
              Time=rethinking::standardize(as.numeric(sim_data_table$Time)) ,
              Dtype = as.numeric(as.factor(sim_data_table$Dtypex)) -1)
  
  m_pois3 <- ulam(
    alist(
      Altai ~ dpois(lambda1),
      Chagyrskaya ~ dpois(lambda2),
      Denisova ~ dpois(lambda3),
      Vindija ~ dpois(lambda4),
      log(lambda1) <- a_1[popul] + b_cov1*Cov + b_dtype1*Dtype + b_time1*Time,
      log(lambda2) <- a_2[popul] + b_cov2*Cov + b_dtype2*Dtype + b_time2*Time,
      log(lambda3) <- a_3[popul] + b_cov3*Cov + b_dtype3*Dtype + b_time3*Time,
      log(lambda4) <- a_4[popul] + b_cov4*Cov + b_dtype4*Dtype + b_time4*Time,
      a_1[popul] ~ dnorm(0,1.5),
      a_2[popul] ~ dnorm(0,1.5),
      a_3[popul] ~ dnorm(0,1.5),
      a_4[popul] ~ dnorm(0,1.5),
      c(b_cov1,b_cov2,b_cov3,b_cov4) ~ dnorm(0,1),
      c(b_dtype1,b_dtype2,b_dtype3,b_dtype4) ~ dnorm(0,1),
      c(b_time1,b_time2,b_time3,b_time4) ~ dnorm(0,1)
    ), data=dat , chains=4 , cores=4 ,log_lik = T)
  return(m_pois3)
}

posterior_predictions_intercept <- function(model,data,true_props){
  # raw data
  sim_data_table <- data %>%
  mutate(ancestryx = ifelse(ancestryx == 1,"Altai",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 2,"Chagyrskaya",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 3,"Denisova",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 4,"Vindija",ancestryx)) %>%
  group_by(Dtypex,Time,Covx,ancestryx,Segx) %>% summarise(count = n()) %>%
  pivot_wider(names_from = ancestryx, values_from = count) %>% replace(is.na(.), 0)
  
  sim_data_table_sumx <- sim_data_table %>% mutate(n_seg = sum(c_across(Altai:Vindija))) %>%
  mutate(ALT = Altai / n_seg,
         CHA = Chagyrskaya / n_seg,
         DEN = Denisova / n_seg,
         VIN = Vindija / n_seg) %>% dplyr::rename(Pop = Segx)
  
  
  sim_data_table_sum = sim_data_table_sumx %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  
  sim_data_table_sum = sim_data_table_sumx %>% dplyr::group_by(Pop)  %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% 
    left_join(sim_data_table_sum,.,by=c("Reference","Pop")) %>% mutate(HDPI_lower = Mean - Sd,HDPI_upper = Mean + Sd) %>% 
    dplyr::select(-Sd) %>% mutate(Data = "raw")
  
  # model data
  post <- extract.samples(model)
  
  pop_intercepts <- list()
  for(grp in 1:length(unique(sim_data_table_sumx$Pop))){
    pop_intercepts[[grp]] <-   matrix(sapply(1:length(post$a_1[,1]), function(i) c(
      exp(post$a_1[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_2[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_3[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_4[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])))),ncol = 4,byrow = T)
  
  }
  
  pop_int <- c()
  for(grp in 1:length(pop_intercepts)){
    pop_int <- rbind(pop_int,data.frame(grp,pop_intercepts[[grp]]))
  }
  colnames(pop_int) <- c("Pop","ALT","CHA","DEN","VIN")
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop")) %>% 
    mutate(HDPI_lower = as.numeric(HDPI_lower))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop")) %>% 
    mutate(HDPI_upper = as.numeric(HDPI_upper)) %>% mutate(Data = "modeled")
  
  True_prop = data.frame(Reference = c("ALT","CHA","DEN","VIN"),True_prop = true_props)
  
  pop_intercept_summary <- rbind(pop_intercept_summary,sim_data_table_sum) %>% left_join(.,True_prop)
  
  
  
  return(pop_intercept_summary)
}

posterior_predictions_diff_from_pivot <- function(model,data){
  # raw data
  sim_data_table <- data %>%
  mutate(ancestryx = ifelse(ancestryx == 1,"Altai",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 2,"Chagyrskaya",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 3,"Denisova",ancestryx)) %>%
  mutate(ancestryx = ifelse(ancestryx == 4,"Vindija",ancestryx)) %>%
  group_by(Dtypex,Time,Covx,ancestryx,Segx) %>% summarise(count = n()) %>%
  pivot_wider(names_from = ancestryx, values_from = count) %>% replace(is.na(.), 0)
  
  sim_data_table_sumx <- sim_data_table %>% mutate(n_seg = sum(c_across(Altai:Vindija))) %>%
  mutate(ALT = Altai / n_seg,
         CHA = Chagyrskaya / n_seg,
         DEN = Denisova / n_seg,
         VIN = Vindija / n_seg) %>% dplyr::rename(Pop = Segx)
  
  
  sim_data_table_mean = sim_data_table_sumx %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) 
  
  
  sim_data_table_sd= sim_data_table_sumx %>% dplyr::group_by(Pop)  %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) 
  sim_data_samples <- list()
  for(grp in 1:length(unique(sim_data_table_sumx$Pop))){
    sim_data_samples[[grp]] <-   matrix(c(
      rnorm(1000,as.numeric(sim_data_table_mean[grp,2]),as.numeric(sim_data_table_sd[grp,2])),
      rnorm(1000,as.numeric(sim_data_table_mean[grp,3]),as.numeric(sim_data_table_sd[grp,3])),
      rnorm(1000,as.numeric(sim_data_table_mean[grp,4]),as.numeric(sim_data_table_sd[grp,4])),
      rnorm(1000,as.numeric(sim_data_table_mean[grp,5]),as.numeric(sim_data_table_sd[grp,5]))),ncol = 4,byrow = F)
    
  }
  
  sim_data_pop_diff <- data.frame((sim_data_samples[[1]] - sim_data_samples[[1]]), pivot = 1,pop = 1)
  for(pvt in 1:length(unique(sim_data_table_sumx$Pop))){
    for(grp in 1:length(unique(sim_data_table_sumx$Pop))){
      sim_data_pop_diff <- rbind(sim_data_pop_diff,data.frame((sim_data_samples[[grp]] - sim_data_samples[[pvt]]), pivot = pvt,pop = grp))
    }
  }
  
  colnames(sim_data_pop_diff) <- c("ALT","CHA","DEN","VIN","PivotPop","Pop")
  for(i in unique(sim_data_pop_diff$PivotPop)){
    sim_data_pop_diff$PivotPop[sim_data_pop_diff$PivotPop == i] <- i
    sim_data_pop_diff$Pop[sim_data_pop_diff$Pop == i] <- i
    
  }
  
  sim_data_pop_diff_summary = sim_data_pop_diff %>% dplyr::group_by(PivotPop,Pop) %>%  dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  sim_data_pop_diff_summary = sim_data_pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% left_join(sim_data_pop_diff_summary,.,by=c("Reference","Pop","PivotPop")) %>% 
    mutate(HDPI_lower = Mean - Sd,HDPI_upper = Mean + Sd) %>% 
    dplyr::select(-Sd) %>% mutate(Data = "raw")

  # model data
  post <- extract.samples(model)
  
  pop_intercepts <- list()
  for(grp in 1:n_groups){
    pop_intercepts[[grp]] <-   matrix(sapply(1:length(post$a_1[,1]), function(i) c(
      exp(post$a_1[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_2[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_3[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_4[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])))),ncol = 4,byrow = T)
  
  }



  pop_diff <- data.frame((pop_intercepts[[1]] - pop_intercepts[[1]]), pivot = 1,pop = 1)
  for(pvt in 1:length(unique(sim_data_table_sumx$Pop))){
    for(grp in 1:length(unique(sim_data_table_sumx$Pop))){
      pop_diff <- rbind(pop_diff,data.frame((pop_intercepts[[grp]] - pop_intercepts[[pvt]]), pivot = pvt,pop = grp))
    }
  }
  
  colnames(pop_diff) <- c("ALT","CHA","DEN","VIN","PivotPop","Pop")
  for(i in unique(pop_diff$PivotPop)){
    pop_diff$PivotPop[pop_diff$PivotPop == i] <- i
    pop_diff$Pop[pop_diff$Pop == i] <- i
    
  }
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop) %>%  dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop"))%>% mutate(HDPI_lower = as.numeric(HDPI_lower))
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop")) %>% mutate(HDPI_upper = as.numeric(HDPI_upper)) %>% mutate(Data = "modeled")

  
  pop_diff_summary_all <- rbind(pop_diff_summary,sim_data_pop_diff_summary) 
  
  return(pop_diff_summary_all)

}

## plot results for model 1

# No diff

No_differnces_res1 <- run_model1(data = No_differnces)


No_differnces_res1_diff <- posterior_predictions_diff_from_pivot(No_differnces_res1,No_differnces)  %>%
  mutate(Pop = as.factor(Pop))

P_No_differnces_res1_diff <- No_differnces_res1_diff %>%
  filter(PivotPop == 1, PivotPop != Pop) %>%
  ggplot(., aes(x=Pop, y=Mean,col=Data)) +
  geom_point() + 
  geom_errorbar(aes(ymin=HDPI_lower, ymax=HDPI_upper), width=.2) +
  geom_hline(yintercept = 0) +
  facet_wrap(~Reference) +
  coord_flip() + 
  xlab("")  + 
  ylab("Difference in Probability") +
  ggtitle("No difference \nbetween populations")


# Diff in Groups

Differnces_in_groups_res1 <- run_model1(Differnces_in_groups)

Differnces_in_groups_res1_diff <- posterior_predictions_diff_from_pivot(model = Differnces_in_groups_res1,data = Differnces_in_groups)  %>%
  mutate(Pop = as.factor(Pop))

P_Differnces_in_groups_res1_diff <- Differnces_in_groups_res1_diff %>%
  filter(PivotPop == 1, PivotPop != Pop) %>%
  ggplot(., aes(x=Pop, y=Mean,col=Data)) +
  geom_point() + 
  geom_errorbar(aes(ymin=HDPI_lower, ymax=HDPI_upper), width=.2) +
  geom_hline(yintercept = 0) +
  facet_wrap(~Reference) +
  coord_flip() + 
  xlab("")  + 
  ylab("Difference in Probability") +
  ggtitle("Real difference \nbetween populations")


# Diff in DT

Differnces_in_DT_res1 <- run_model1(Differnces_in_DT)

Differnces_in_DT_res1_diff <- posterior_predictions_diff_from_pivot(model = Differnces_in_DT_res1,data = Differnces_in_DT)  %>%
  mutate(Pop = as.factor(Pop))


P_Differnces_in_DT_res1_diff <- Differnces_in_DT_res1_diff %>%
  filter(PivotPop == 1, PivotPop != Pop) %>%
  ggplot(., aes(x=Pop, y=Mean,col=Data)) +
  geom_point() + 
  geom_errorbar(aes(ymin=HDPI_lower, ymax=HDPI_upper), width=.2) +
  geom_hline(yintercept = 0) +
  facet_wrap(~Reference) +
  coord_flip() + 
  xlab("")  + 
  ylab("Difference in Probability") +
  ggtitle("Differnce caused by \ndata type")

# Diff in DT and Cov

Differnces_in_Cov_DT_res1 <- run_model1(Differnces_in_Cov_DT)

Differnces_in_Cov_DT_res1_diff <- posterior_predictions_diff_from_pivot(model = Differnces_in_Cov_DT_res1,data = Differnces_in_Cov_DT)  %>%
  mutate(Pop = as.factor(Pop))


P_Differnces_in_Cov_DT_res1_diff <- Differnces_in_Cov_DT_res1_diff %>%
  filter(PivotPop == 1, PivotPop != Pop) %>%
  ggplot(., aes(x=Pop, y=Mean,col=Data)) +
  geom_point() + 
  geom_errorbar(aes(ymin=HDPI_lower, ymax=HDPI_upper), width=.2) +
  geom_hline(yintercept = 0) +
  facet_wrap(~Reference) +
  coord_flip() + 
  xlab("")  + 
  ylab("Difference in Probability") +
  ggtitle("Differnce caused by \ncoverage and data type")

save(No_differnces_res1_diff,Differnces_in_groups_res1_diff,Differnces_in_DT_res1_diff,Differnces_in_Cov_DT_res1_diff,file = paste0(folder_path_data,"P_TV_Lineage_Assignment_GLM_sim_testing_model1_data.RData"))

```



###### Real Data application

Now we apply the model to real data. We use stan to fit it.

```{r echo=FALSE, eval=FALSE, results='hide'}
## RUN ON CLUSTER


library(rethinking)
library(tidyverse)

load(paste0(folder_path_data,"MatchingRates_file_all_anno_class_BT_non_overlapping_",genetic_map,".RData"))

MatchingRates_file_anno_class_multnomial_data <- MatchingRates_file_anno_class_BT_non_overlapping %>%
  filter(Highest_matching_archaic %in% c("Altai", "Chagyrskaya", "Denisova", "Vindija33.19"))

# model 1 Superpop

MatchingRates_file_anno_class_poisson_data_table_Supercluster <- MatchingRates_file_anno_class_multnomial_data %>%
  mutate(Highest_matching_archaic = ifelse(Highest_matching_archaic == "Vindija33.19","Vindija",Highest_matching_archaic)) %>%
  group_by(Highest_matching_archaic,superpopulation_cluster,Cov,DataType,ML_BP_Mean,ave_cont,Sites) %>% summarise(count = n()) %>%
  pivot_wider(names_from = Highest_matching_archaic, values_from = count) %>% replace(is.na(.), 0)

dat_Superpop.1 <- list(popul =  as.numeric(as.factor(MatchingRates_file_anno_class_poisson_data_table_Supercluster$superpopulation_cluster)),
            Altai = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Supercluster$Altai),
            Chagyrskaya = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Supercluster$Chagyrskaya),
            Denisova = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Supercluster$Denisova),
            Vindija = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Supercluster$Vindija),
            Dtype = as.numeric(as.factor(MatchingRates_file_anno_class_poisson_data_table_Supercluster$DataType)) -1,
            Sites = as.numeric(as.factor(MatchingRates_file_anno_class_poisson_data_table_Supercluster$Sites)) -1)

Poisson_seg_assignment_comp_Superpop.1 <- ulam(
  alist(
    Altai ~ dpois(lambda1),
    Chagyrskaya ~ dpois(lambda2),
    Denisova ~ dpois(lambda3),
    Vindija ~ dpois(lambda4),
    log(lambda1) <- a_1[popul] + b_dtype1*Dtype + b_sites1*Sites,
    log(lambda2) <- a_2[popul] + b_dtype2*Dtype + b_sites2*Sites,
    log(lambda3) <- a_3[popul] + b_dtype3*Dtype + b_sites3*Sites,
    log(lambda4) <- a_4[popul] + b_dtype3*Dtype + b_sites4*Sites,
    a_1[popul] ~ dnorm(0,1.5),
    a_2[popul] ~ dnorm(0,1.5),
    a_3[popul] ~ dnorm(0,1.5),
    a_4[popul] ~ dnorm(0,1.5),
    c(b_dtype1,b_dtype2,b_dtype3,b_dtype4) ~ dnorm(0,1),
    c(b_sites1,b_sites2,b_sites3,b_sites4) ~ dnorm(0,1)
  ), data=dat_Superpop.1 , chains=6 , cores=6,iter = 4000 ,log_lik = T)

MatchingRates_file_anno_class_poisson_data_table_Popcluster <- MatchingRates_file_anno_class_multnomial_data %>%
  mutate(Highest_matching_archaic = ifelse(Highest_matching_archaic == "Vindija33.19","Vindija",Highest_matching_archaic)) %>%
  group_by(Highest_matching_archaic,(!!sym(cluster_used)),Cov,DataType,ML_BP_Mean,ave_cont,Sites) %>% summarise(count = n()) %>%
  pivot_wider(names_from = Highest_matching_archaic, values_from = count) %>% replace(is.na(.), 0)

dat_Pop.1 <- list(popul =  as.numeric(as.factor(MatchingRates_file_anno_class_poisson_data_table_Popcluster[[cluster_used]])),
            Altai = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Popcluster$Altai),
            Chagyrskaya = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Popcluster$Chagyrskaya),
            Denisova = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Popcluster$Denisova),
            Vindija = as.numeric(MatchingRates_file_anno_class_poisson_data_table_Popcluster$Vindija),
            Dtype = as.numeric(as.factor(MatchingRates_file_anno_class_poisson_data_table_Popcluster$DataType)) -1,
            Sites = as.numeric(as.factor(MatchingRates_file_anno_class_poisson_data_table_Popcluster$Sites)) -1)

Poisson_seg_assignment_comp_Pop.1 <- ulam(
  alist(
    Altai ~ dpois(lambda1),
    Chagyrskaya ~ dpois(lambda2),
    Denisova ~ dpois(lambda3),
    Vindija ~ dpois(lambda4),
    log(lambda1) <- a_1[popul] + b_dtype1*Dtype + b_sites1*Sites,
    log(lambda2) <- a_2[popul] + b_dtype2*Dtype + b_sites2*Sites,
    log(lambda3) <- a_3[popul] + b_dtype3*Dtype + b_sites3*Sites,
    log(lambda4) <- a_4[popul] + b_dtype3*Dtype + b_sites4*Sites,
    a_1[popul] ~ dnorm(0,1.5),
    a_2[popul] ~ dnorm(0,1.5),
    a_3[popul] ~ dnorm(0,1.5),
    a_4[popul] ~ dnorm(0,1.5),
    c(b_dtype1,b_dtype2,b_dtype3,b_dtype4) ~ dnorm(0,1),
    c(b_sites1,b_sites2,b_sites3,b_sites4) ~ dnorm(0,1)
  ), data=dat_Pop.1 , chains=6 , cores=6 ,iter = 4000,log_lik = T)


```

```{r echo=FALSE, eval=TRUE, results='hide'}



plotting_GLM_model_fn <- function(model_superpop,data_superpop,model_pop,data_pop,cluster_used){
  
  ### plotting Supercluster ###
  post <- extract.samples(model_superpop)
  
  pop_intercepts <- list()
  for(grp in 1:length(unique(data_superpop$superpopulation_cluster))){
    pop_intercepts[[grp]] <-   matrix(sapply(1:length(post$a_1[,1]), function(i) c(
      exp(post$a_1[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_2[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_3[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_4[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])))),ncol = 4,byrow = T)
  
  }
  
  pop_int <- c()
  for(grp in 1:length(unique(data_superpop$superpopulation_cluster))){
    pop_int <- rbind(pop_int,data.frame(levels(as.factor(data_superpop$superpopulation_cluster))[grp],pop_intercepts[[grp]]))
  }
  colnames(pop_int) <- c("Pop","ALT","CHA","DEN","VIN")
  
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary_Superpop <- pop_intercept_summary
  
  
  pop_diff <- data.frame((pop_intercepts[[1]] - pop_intercepts[[1]]), pivot = 1,pop = 1)
  for(pvt in 1:length(unique(data_superpop$superpopulation_cluster))){
    for(grp in 1:length(unique(data_superpop$superpopulation_cluster))){
      pop_diff <- rbind(pop_diff,data.frame((pop_intercepts[[grp]] - pop_intercepts[[pvt]]), pivot = pvt,pop = grp))
    }
  }
  
  colnames(pop_diff) <- c("ALT","CHA","DEN","VIN","PivotPop","Pop")
  #pop_diff = pop_diff %>% filter(PivotPop != Pop)
  for(i in unique(pop_diff$PivotPop)){
    pop_diff$PivotPop[pop_diff$PivotPop == i] <- levels(as.factor(data_superpop$superpopulation_cluster))[i]
    pop_diff$Pop[pop_diff$Pop == i] <- levels(as.factor(data_superpop$superpopulation_cluster))[i]
    
  }
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop) %>%  dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop"))
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop"))
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop"))
  
  pop_diff_summary_Superpop <- pop_diff_summary
  
  
  ### plotting Popcluster ###
  post <- extract.samples(model_pop)
  
  pop_intercepts <- list()
  for(grp in 1:length(unique(data_pop[[cluster_used]]))){
    pop_intercepts[[grp]] <-   matrix(sapply(1:length(post$a_1[,1]), function(i) c(
      exp(post$a_1[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_2[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_3[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_4[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])))),ncol = 4,byrow = T)
  
  }
  
  pop_int <- c()
  for(grp in 1:length(unique(data_pop[[cluster_used]]))){
    pop_int <- rbind(pop_int,data.frame(levels(as.factor(data_pop[[cluster_used]]))[grp],pop_intercepts[[grp]]))
  }
  colnames(pop_int) <- c("Pop","ALT","CHA","DEN","VIN")
  
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary_Pop <- pop_intercept_summary
  
  pop_diff <- data.frame((pop_intercepts[[1]] - pop_intercepts[[1]]), pivot = 1,pop = 1)
  for(pvt in 1:length(unique(data_pop[[cluster_used]]))){
    for(grp in 1:length(unique(data_pop[[cluster_used]]))){
      pop_diff <- rbind(pop_diff,data.frame((pop_intercepts[[grp]] - pop_intercepts[[pvt]]), pivot = pvt,pop = grp))
    }
  }
  
  colnames(pop_diff) <- c("ALT","CHA","DEN","VIN","PivotPop","Pop")
  #pop_diff = pop_diff %>% filter(PivotPop != Pop)
  for(i in unique(pop_diff$PivotPop)){
    pop_diff$PivotPop[pop_diff$PivotPop == i] <- levels(as.factor(data_pop[[cluster_used]]))[i]
    pop_diff$Pop[pop_diff$Pop == i] <- levels(as.factor(data_pop[[cluster_used]]))[i]
    
  }
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop) %>%  dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop"))
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop"))
  
  pop_diff_summary = pop_diff %>% dplyr::group_by(PivotPop,Pop)  %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_diff_summary,.,by=c("Reference","Pop","PivotPop"))
  
  pop_diff_summary_Pop <- pop_diff_summary
  return(list(pop_intercept_summary_Superpop,pop_diff_summary_Superpop,pop_intercept_summary_Pop,pop_diff_summary_Pop))
}

plot_data_m.1 <- plotting_GLM_model_fn(model_superpop = Poisson_seg_assignment_comp_Superpop.1,data_superpop = MatchingRates_file_anno_class_poisson_data_table_Supercluster,model_pop = 
                                        Poisson_seg_assignment_comp_Pop.1,data_pop = MatchingRates_file_anno_class_poisson_data_table_Popcluster,cluster_used = cluster_used)


save(plot_data_m.1,file = paste0(folder_path_data,"Pois_model_summary_bootstrap_non_overlapping_",genetic_map,".RData"))

## parameter est

Superpop_list <- c()
for(grp in 1:length(unique(MatchingRates_file_anno_class_poisson_data_table_Supercluster$superpopulation_cluster))){
  for(arch in c("ALT","CHA","DEN","VIN")){
    Superpop_list <- c(Superpop_list,paste0(levels(as.factor(MatchingRates_file_anno_class_poisson_data_table_Supercluster$superpopulation_cluster))[grp],"_raw_prop_",arch))
  }
}

Pop_list <- c()
for(grp in 1:length(unique(MatchingRates_file_anno_class_poisson_data_table_Popcluster$clusterF3D_broad))){
  for(arch in c("ALT","CHA","DEN","VIN")){
    Pop_list <- c(Pop_list,paste0(levels(as.factor(MatchingRates_file_anno_class_poisson_data_table_Popcluster$clusterF3D_broad))[grp],"_raw_prop_",arch))
  }
}

Param_est_res_Superpop_m.1 <- as.data.frame(as.matrix(precis(Poisson_seg_assignment_comp_Superpop.1,2))) %>% 
  rownames_to_column("Parameter") %>% 
  mutate(Parameter = c(Superpop_list,"b_dtype_ALT","b_dtype_CHA","b_dtype_DEN","b_dtype_VIN","b_sites_ALT","b_sites_CHA","b_sites_DEN","b_sites_VIN")) %>% 
  mutate(cluster = "Superpopulation")
Param_est_res_pop_m.1 <- as.data.frame(as.matrix(precis(Poisson_seg_assignment_comp_Pop.1,2))) %>% 
  rownames_to_column("Parameter") %>% 
  mutate(Parameter = c(Pop_list,"b_dtype_ALT","b_dtype_CHA","b_dtype_DEN","b_dtype_VIN","b_sites_ALT","b_sites_CHA","b_sites_DEN","b_sites_VIN")) %>% 
  mutate(cluster = "Population")

Param_est_res_m.1 <- rbind(Param_est_res_Superpop_m.1,Param_est_res_pop_m.1)

write.csv(Param_est_res_m.1,paste0(folder_path_Sup_Tables,"Parameter_estimatesGLM_m1.csv"),quote = F,row.names = T)


```

```{r echo=FALSE, eval=TRUE, results='hide'}



plotting_GLM_model_intercept_fn <- function(model_superpop,data_superpop,model_pop,data_pop,cluster_used){
  
  ### plotting Supercluster ###
  post <- extract.samples(model_superpop)
  
  pop_intercepts <- list()
  for(grp in 1:length(unique(data_superpop$superpopulation_cluster))){
    pop_intercepts[[grp]] <-   matrix(sapply(1:length(post$a_1[,1]), function(i) c(
      exp(post$a_1[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_2[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_3[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_4[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])))),ncol = 4,byrow = T)
  
  }
  
  pop_int <- c()
  for(grp in 1:length(unique(data_superpop$superpopulation_cluster))){
    pop_int <- rbind(pop_int,data.frame(levels(as.factor(data_superpop$superpopulation_cluster))[grp],pop_intercepts[[grp]]))
  }
  colnames(pop_int) <- c("Pop","ALT","CHA","DEN","VIN")
  
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary_Superpop <- pop_intercept_summary
  
  
  pop_intercept_summary_all = data.frame(Reference = c("ALT","CHA","DEN","VIN"),
                                         All_Mean = c(mean(pop_int$ALT),mean(pop_int$CHA),mean(pop_int$DEN),mean(pop_int$VIN)),
                                         All_Sd =   c(sd(pop_int$ALT),sd(pop_int$CHA),sd(pop_int$DEN),sd(pop_int$VIN)),
                                         All_HDPI_lower = c(HPDI(pop_int$ALT, 0.95)[1],HPDI(pop_int$CHA, 0.95)[1],HPDI(pop_int$DEN, 0.95)[1],HPDI(pop_int$VIN, 0.95)[1]),
                                         All_HDPI_upper = c(HPDI(pop_int$ALT, 0.95)[2],HPDI(pop_int$CHA, 0.95)[2],HPDI(pop_int$DEN, 0.95)[2],HPDI(pop_int$VIN, 0.95)[2]))
  
  pop_intercept_summary_Superpop <- inner_join(x=pop_intercept_summary_Superpop,y=pop_intercept_summary_all,by="Reference")
  
  ### plotting Popcluster ###
  post <- extract.samples(model_pop)
  
  pop_intercepts <- list()
  for(grp in 1:length(unique(data_pop[[cluster_used]]))){
    pop_intercepts[[grp]] <-   matrix(sapply(1:length(post$a_1[,1]), function(i) c(
      exp(post$a_1[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_2[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_3[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])),
      exp(post$a_4[i,grp])/(exp(post$a_1[i,grp])+exp(post$a_2[i,grp])+exp(post$a_3[i,grp])+exp(post$a_4[i,grp])))),ncol = 4,byrow = T)
  
  }
  
  pop_int <- c()
  for(grp in 1:length(unique(data_pop[[cluster_used]]))){
    pop_int <- rbind(pop_int,data.frame(levels(as.factor(data_pop[[cluster_used]]))[grp],pop_intercepts[[grp]]))
  }
  colnames(pop_int) <- c("Pop","ALT","CHA","DEN","VIN")
  
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = mean(ALT),
                                                                                     CHA = mean(CHA),
                                                                                     DEN = mean(DEN),
                                                                                     VIN = mean(VIN)) %>% pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Mean")
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = sd(ALT),
                                                                                     CHA = sd(CHA),
                                                                                     DEN = sd(DEN),
                                                                                     VIN = sd(VIN)) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "Sd") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[1],
                                                                                     CHA = HPDI(CHA, 0.95)[1],
                                                                                     DEN = HPDI(DEN, 0.95)[1],
                                                                                     VIN = HPDI(VIN, 0.95)[1]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_lower")  %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary = pop_int %>% dplyr::group_by(Pop) %>% dplyr::summarize(ALT = HPDI(ALT, 0.95)[2],
                                                                                     CHA = HPDI(CHA, 0.95)[2],
                                                                                     DEN = HPDI(DEN, 0.95)[2],
                                                                                     VIN = HPDI(VIN, 0.95)[2]) %>% 
    pivot_longer(c(ALT,CHA,DEN,VIN),names_to="Reference",values_to = "HDPI_upper") %>% left_join(pop_intercept_summary,.,by=c("Reference","Pop"))
  
  pop_intercept_summary_Pop <- pop_intercept_summary
  
  pop_intercept_summary_Pop <- pop_intercept_summary
  
  
  
  pop_intercept_summary_all = data.frame(Reference = c("ALT","CHA","DEN","VIN"),
                                         All_Mean = c(mean(pop_int$ALT),mean(pop_int$CHA),mean(pop_int$DEN),mean(pop_int$VIN)),
                                         All_Sd =   c(sd(pop_int$ALT),sd(pop_int$CHA),sd(pop_int$DEN),sd(pop_int$VIN)),
                                         All_HDPI_lower = c(HPDI(pop_int$ALT, 0.95)[1],HPDI(pop_int$CHA, 0.95)[1],HPDI(pop_int$DEN, 0.95)[1],HPDI(pop_int$VIN, 0.95)[1]),
                                         All_HDPI_upper = c(HPDI(pop_int$ALT, 0.95)[2],HPDI(pop_int$CHA, 0.95)[2],HPDI(pop_int$DEN, 0.95)[2],HPDI(pop_int$VIN, 0.95)[2]))
  
  pop_intercept_summary_Pop <- inner_join(pop_intercept_summary_Pop,pop_intercept_summary_all,by="Reference")
  
  return(list(pop_intercept_summary_Superpop,pop_intercept_summary_Pop))
}

plot_data_m.1 <- plotting_GLM_model_intercept_fn(model_superpop = Poisson_seg_assignment_comp_Superpop.1,data_superpop = MatchingRates_file_anno_class_poisson_data_table_Supercluster,model_pop = 
                                        Poisson_seg_assignment_comp_Pop.1,data_pop = MatchingRates_file_anno_class_poisson_data_table_Popcluster,cluster_used = cluster_used)

plot_data_m.2 <- plotting_GLM_model_intercept_fn(model_superpop = Poisson_seg_assignment_comp_Superpop.2,data_superpop = MatchingRates_file_anno_class_poisson_data_table_Supercluster,model_pop = 
                                      Poisson_seg_assignment_comp_Pop.2,data_pop = MatchingRates_file_anno_class_poisson_data_table_Popcluster,cluster_used = cluster_used)

plot_data_m.3 <- plotting_GLM_model_intercept_fn(Poisson_seg_assignment_comp_Superpop.3,MatchingRates_file_anno_class_poisson_data_table_Supercluster,
                                       Poisson_seg_assignment_comp_Pop.3,MatchingRates_file_anno_class_poisson_data_table_Popcluster,cluster_used = cluster_used)

plot_data_m.4 <- plotting_GLM_model_intercept_fn(Poisson_seg_assignment_comp_Superpop.4,MatchingRates_file_anno_class_poisson_data_table_Supercluster,
                                       Poisson_seg_assignment_comp_Pop.4,MatchingRates_file_anno_class_poisson_data_table_Popcluster,cluster_used = cluster_used)

#save(plot_data_m.1,plot_data_m.2,plot_data_m.3,plot_data_m.4,file = paste0("~/EMH_Introgression_Project/Introgression_Detection/Phylogenetic_fragment_analysis/Bayesian_GLM_modelling/Pois_model_summary_bootstrap",DiagSites,".RData"))


save(plot_data_m.1,plot_data_m.2,plot_data_m.3,plot_data_m.4,file = paste0(folder_path_data,"Pois_model_summary_bootstrap_non_overlapping_intercept_only",genetic_map,"_",DiagSites,".RData"))

```

##### Fitting one or two normal distributions

We can also look at the mismatch distribution to the Chagyrskaya Neandertal and estimate if one or two distributions give a better fit. We restrict to segments with at least 15 sites that are informative to reduce the noise in the data. Furthermore we constrain to segments with a match rate of 0.3 to reduce potentail false positives. To fit two distribution a simple EM is used. To prevent numerical underflow we set 0 value to a small number of 1e-12.

##### For match to Chagyrskaya

```{r echo=FALSE, eval=TRUE, results='hide'}
# Likelihood function for a single Gaussian distribution
match_rates_likelihood_single <- function(params, data) {
  mu <- params[1]
  sigma <- params[2]
  
  log_likelihood <- sum(log(dnorm(data, mean = mu, sd = sigma)))
  
  return(-log_likelihood) # Minimize negative log-likelihood
}

# Likelihood function for a mixture of two Gaussian distributions
match_rates_likelihood_mixture <- function(params, data) {
  mu1 <- params[1]
  mu2 <- params[2]
  sigma1 <- params[3]
  sigma2 <- params[4]
  weight <- params[5]
  
  log_likelihood <- sum(log(weight * dnorm(data, mean = mu1, sd = sigma1) + (1 - weight) * dnorm(data, mean = mu2, sd = sigma2)))
  
  return(-log_likelihood) # Minimize negative log-likelihood
}


# EM algorithm for fitting a mixture of two Gaussian distributions to the match rate
EM_algorithm <- function(data, initial_params, max_iter = 1000, tol = 1e-8,constrain = F) {
  n <- length(data)
  
  # parameters
  mu <- initial_params$mu
  sigma <- initial_params$sigma
  weight <- initial_params$weight
  
  
  for (iter in 1:max_iter) {

    # E-step: Calculate responsibilities
    responsibility1 <- weight * dnorm(data, mean = mu[1], sd = sigma[1])
    responsibility2 <- (1 - weight) * dnorm(data, mean = mu[2], sd = sigma[2])
    total_responsibility <- responsibility1 + responsibility2
    responsibility1 <- responsibility1 / total_responsibility
    
    
    # M-step: Update parameters
    prev_mu <- mu
    prev_sigma <- sigma
    prev_weight <- weight
    
    # Update means
    mu[1] <- sum(responsibility1 * data) / sum(responsibility1)
    mu[2] <- sum((1 - responsibility1) * data) / sum(1 - responsibility1)
    

    # Update standard deviations
    sigma[1] <- sqrt(sum(responsibility1 * (data - mu[1])^2) / sum(responsibility1))
    sigma[2] <- sqrt(sum((1 - responsibility1) * (data - mu[2])^2) / sum(1 - responsibility1))
    
    # Update weight
    weight <- mean(responsibility1)
    
    # Constrain means to meet the conditions: mu_i + 3*sigma_i < 1 and mu_i - 3*sigma_i > 0
    if(constrain){
      sigma[1] <- ifelse(sigma[1] == 0,1e-12,sigma[2])
      sigma[2] <- ifelse(sigma[2] == 0,1e-12,sigma[2])
    }
    
    
    # Check for convergence
    if (max(abs(mu - prev_mu)) < tol && max(abs(sigma - prev_sigma)) < tol && abs(weight - prev_weight) < tol) {
      break
    }
  }
  
  # Compute log-likelihood
  log_likelihood <- sum(log(weight * dnorm(data, mean = mu[1], sd = sigma[1]) + (1 - weight) * dnorm(data, mean = mu[2], sd = sigma[2])))
  
  return(list(par = c(mu[1],mu[2], sigma[1],sigma[2],  weight), log_likelihood = -log_likelihood))
}


# Likelihood Ratio Test
LRT <- function(LL_H0, LL_H1,n_param_H0,n_param_H1) {
  lr <- 2 * (LL_H0 - LL_H1)
  p_value <- 1 - pchisq(lr, df = n_param_H1 - n_param_H0)
  
  return(list(LR = lr, p_value = p_value))
}

#### fitting Only Variable

MatchingRates_file_anno_red_OnlyVariable <- MatchingRates_file_all_anno %>% 
  filter(chrom != "X", sample != "LeangPanninge") %>% 
  filter(count_Chagyrskaya >= 15) %>%
  filter(Sites == "OnlyVariable") %>%
  filter(BT_prop_pHcount_matching_Chagyrskaya_mean >= 0.3)



MatchingRates_file_anno_red_OnlyVariable_non_overlapping_list = MatchingRates_file_anno_red_OnlyVariable %>%
  group_overlaps_fn(.)  %>% 
  group_by(!!sym(cluster_used),group_id) %>% 
  dplyr::summarize(sampled_frag = sample(x = frag_ID,size = 1,replace = T)) %>% ungroup() 
  
MatchingRates_file_anno_red_OnlyVariable_non_overlapping <- inner_join(MatchingRates_file_anno_red_OnlyVariable_non_overlapping_list[,c("sampled_frag")],
                                                                       MatchingRates_file_anno_red_OnlyVariable,by=c("sampled_frag"="frag_ID"))  %>% 
  dplyr::select(clusterF3D_broad,BT_prop_pHcount_matching_Chagyrskaya_mean,DataType) %>%
  dplyr::rename(value = BT_prop_pHcount_matching_Chagyrskaya_mean)


comb_x <- MatchingRates_file_anno_red_OnlyVariable_non_overlapping %>% distinct(clusterF3D_broad,DataType)

res_fit <- list()
plot_data_l <- list()
for(i in 1:length(comb_x$clusterF3D_broad)){
  print(comb_x$clusterF3D_broad[i])
  data_x <- MatchingRates_file_anno_red_OnlyVariable_non_overlapping %>% 
    filter(clusterF3D_broad == comb_x$clusterF3D_broad[i], DataType == comb_x$DataType[i]) %>% drop_na()

  # Initial parameter values for optimization
  initial_params_single <- c(mean(data_x$value), sd(data_x$value))
  initial_params_mixture <- c(0.3, 0.7, 0.1, 0.1, 0.5)
  initial_params_EM_fit <- list(mu = c(0.3, 0.7), sigma = c(0.1, 0.1), weight = 0.5)
  
  # Run EM algorithm
  fit_mixture_EM <- EM_algorithm(data_x$value, initial_params_EM_fit,constrain = T)
  
  # Optimize parameters for the single Gaussian distribution
  fit_single <- optim(initial_params_single, match_rates_likelihood_single, data = data_x$value)
  
  # Optimize parameters for the mixture of two Gaussian distributions
  #fit_mixture <- optim(initial_params_mixture, match_rates_likelihood_mixture, data = data_x$value,lower = c(0,0,0,0,0),upper = c(1,1,1,1,1),method = "L-BFGS-B")
  
  lrt_result <- LRT(fit_single$value, fit_mixture_EM$log_likelihood,length(fit_single$par), length(fit_mixture_EM$par))

  
  res_fit[[i]] <- data.frame(pop = unique(data_x$clusterF3D_broad),DataType = unique(data_x$DataType),mu_single = fit_single$par[1], sd_single = fit_single$par[2], LL_single = fit_single$value,
                             mu1_mixture = fit_mixture_EM$par[1],mu2_mixture = fit_mixture_EM$par[2],sd1_mixture = fit_mixture_EM$par[3],
                             sd2_mixture = fit_mixture_EM$par[4],weight_mixture = fit_mixture_EM$par[5],LL_mixture = fit_mixture_EM$log_likelihood,
                             LR = lrt_result$LR,p_value = lrt_result$p_value,n_segs = length(data_x$value))
}

Match_fit_OnlyVariable <- do.call("rbind",res_fit) %>% mutate(min_match = 0.3,min_inf_sites = 15)

write.csv(Match_fit_OnlyVariable,paste0(folder_path_Sup_Tables,"Gaussian_fit_to_match_rates_to_Chagyrskaya.csv"),quote = F,row.names = T)


```
